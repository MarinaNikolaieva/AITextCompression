{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oClKvaHEgFSj"
      },
      "outputs": [],
      "source": [
        "#import needed libraries\n",
        "import numpy as np\n",
        "import math\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ShannonEntropy(inputString):\n",
        "  length = len(inputString)\n",
        "  bases = collections.Counter([tmp_base for tmp_base in inputString])\n",
        "  entropy = 0.0\n",
        "  for base in bases:\n",
        "    # number of residues\n",
        "    n_i = bases[base]\n",
        "    # n_i (# residues type i) / M (# residues in column)\n",
        "    p_i = n_i / float(length)\n",
        "    entropy_i = p_i * (math.log(p_i, 2))\n",
        "    entropy += entropy_i\n",
        "  if entropy == 0:\n",
        "    return entropy\n",
        "  return entropy * -1"
      ],
      "metadata": {
        "id": "DN8O-BNciYGW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing libraries for ranking the text and getting the most important words\n",
        "!pip install pytextrank\n",
        "import pytextrank\n",
        "import spacy\n",
        "from icecream import ic"
      ],
      "metadata": {
        "id": "UpoLYmzQCrVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82e475e-98a7-43b0-f425-181373c58f58"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytextrank in /usr/local/lib/python3.10/dist-packages (3.2.5)\n",
            "Requirement already satisfied: graphviz>=0.13 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (0.20.1)\n",
            "Requirement already satisfied: icecream>=2.1 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (2.1.3)\n",
            "Requirement already satisfied: networkx[default]>=2.6 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (3.1)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (1.11.3)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (3.6.1)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from icecream>=2.1->pytextrank) (0.4.6)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from icecream>=2.1->pytextrank) (2.0.0)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from icecream>=2.1->pytextrank) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (1.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0->pytextrank) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0->pytextrank) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the English analyzer\n",
        "engnlp = spacy.load(\"en_core_web_sm\")\n",
        "engnlp.add_pipe(\"textrank\")\n",
        "\n",
        "text = \"Let’s say we have a language model that has been trained with a vocabulary of only 5 words sunny, day, beautiful, scenery, clouds. Now, we want to calculate the perplexity of the model when it sees the phrase beautiful scenery.\"\n",
        "processedText = engnlp(text)\n",
        "\n",
        "sum = 0\n",
        "for phrase in processedText._.phrases:\n",
        "    ic(phrase.rank, phrase.count, phrase.text)\n",
        "    sum = sum + phrase.rank\n",
        "\n",
        "#Phrase rank Average. The higher Average means the text is less likely to be artificial\n",
        "average = sum / len(processedText._.phrases)\n",
        "print(average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUds3fCjYQKD",
        "outputId": "66d7c448-a14c-44d6-f609-2225a81b762c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| phrase.rank: 0.13708962097300056\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'the phrase beautiful scenery'\n",
            "ic| phrase.rank: 0.11032088400982727\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'day, beautiful, scenery, clouds'\n",
            "ic| phrase.rank: 0.07738845091750776\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'a language model'\n",
            "ic| phrase.rank: 0.05657660903281399\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'the model'\n",
            "ic| phrase.rank: 0.05054979272207172\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'a vocabulary'\n",
            "ic| phrase.rank: 0.03857532531560477\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'the perplexity'\n",
            "ic| phrase.rank: 0.037999529333363036\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'only 5 words'\n",
            "ic| phrase.rank: 0.0, phrase.count: 1, phrase.text: 'it'\n",
            "ic| phrase.rank: 0.0, phrase.count: 1, phrase.text: 'that'\n",
            "ic| phrase.rank: 0.0, phrase.count: 2, phrase.text: 'we'\n",
            "ic| phrase.rank: 0.0, phrase.count: 1, phrase.text: '’s'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04622729202765355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "list = []\n",
        "for phrase in processedText._.phrases:\n",
        "    list.append(phrase.rank)\n",
        "\n",
        "#The lower Variance means the text is less likely to be edited\n",
        "print(len(list))\n",
        "print(statistics.variance(list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5zNgeyukBPL",
        "outputId": "4f48ce82-1f96-4300-d91b-fb34befa6e5b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "0.0022134860338990803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the Ukrainian analyzer\n",
        "#!spacy download uk_core_news_sm\n",
        "#ukrnlp = spacy.load(\"uk_core_news_sm\")\n",
        "#ukrnlp.add_pipe(\"textrank\")\n",
        "engnlp = spacy.load(\"en_core_web_sm\")\n",
        "engnlp.add_pipe(\"textrank\")\n",
        "\n",
        "text = \"Маємо речення, яке ми хочемо проаналізувати. Нехай в нас є модель, натренована українською мовою. Нам необхідно отримати список рангів\"\n",
        "processedText = engnlp(text)\n",
        "#The Ukrainian analizer didn't work, but the English one can work with sentences in Ukrainian\n",
        "sum = 0\n",
        "for phrase in processedText._.phrases:\n",
        "    ic(phrase.rank, phrase.count, phrase.text)\n",
        "    sum = sum + phrase.rank\n",
        "\n",
        "#Phrase rank Average. The higher Average means the text is less likely to be artificial\n",
        "average = sum / len(processedText._.phrases)\n",
        "print(average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B5NVW5DbpeG",
        "outputId": "c96850c5-189b-4607-ae2c-3f34466065bf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| phrase.rank: 0.22551339760351258\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'натренована українською мовою'\n",
            "ic| phrase.rank: 0.21552814840638496\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'українською мовою'\n",
            "ic| phrase.rank: 0.19288925400862628\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'хочемо проаналізувати'\n",
            "ic| phrase.rank: 0.18006512794501117\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'Нам необхідно отримати список рангів'\n",
            "ic| phrase.rank: 0.17930495508687905\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'список рангів'\n",
            "ic| phrase.rank: 0.16303438810847415\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'Нехай в нас є модель'\n",
            "ic| phrase.rank: 0.12412561754091966\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'хочемо'\n",
            "ic| phrase.rank: 0.10705477344677523\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'Маємо речення'\n",
            "ic| phrase.rank: 0.09643570488844748\n",
            "    phrase.count: 1\n",
            "    phrase.text: 'яке'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16488348522611448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the next words. Installing libraries\n",
        "#https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "gCKtk725f0Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944f1efe-c668-4f61-db28-05a1ede01b8d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delimiters = [',', '|', ';', '.', ':', ' ']\n",
        "signs = ['\\'', '\\\"', ' ']\n",
        "\n",
        "#Remove last symbol from the string if it's a delimiter. Made for more clear word comparing\n",
        "def ClearElements(elements):\n",
        "  array = []\n",
        "  for element in elements:\n",
        "    if element[-1] in delimiters:\n",
        "        element = element[:-1]\n",
        "    array.append(element)\n",
        "  return array\n",
        "\n",
        "#Gets the array of words. Returns the string made out of these words combined\n",
        "def CombineWordsInString(words):\n",
        "  string = \"\"\n",
        "  for word in words:\n",
        "    if isinstance(word, int):  #If word is a number, it is converted into a string\n",
        "      word = str(word)\n",
        "    string = string + word + \" \"\n",
        "  string = string[:-1]\n",
        "  return string\n",
        "\n",
        "#Clear the string from the symbols put in signs array\n",
        "def ClearString(inputString):\n",
        "  for item in signs:\n",
        "    inputString = inputString.replace(item, '')\n",
        "  return inputString\n",
        "\n",
        "def CombineCharactersToString(inputSymbols):\n",
        "  string = \"\"\n",
        "  for character in range(1, len(inputSymbols), 2):\n",
        "    string = string + inputSymbols[character]\n",
        "  return string"
      ],
      "metadata": {
        "id": "gJhlnqmcHBXF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding and decoding must be reworked as well - it's a different scheme\n",
        "#BUT I won't encode the prompt for now. I'll encode just the sentences\n",
        "\n",
        "def CombineSentencesToString(inputSentences):\n",
        "  outputString = \"\"\n",
        "  for sentence in inputSentences:\n",
        "    outputString = outputString + \" \" + sentence\n",
        "  return outputString"
      ],
      "metadata": {
        "id": "hGLFqQOPFkgI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the words possible to appear after the string we give\n",
        "def FindWordsDictionary(encoded_text, numOfWords, model, tokenizer):\n",
        "  #1. step to get the logits of the next token\n",
        "  with torch.inference_mode():\n",
        "    outputs = model(**encoded_text)\n",
        "  next_token_logits = outputs.logits[0, -1, :]\n",
        "  # 2. step to convert the logits to probabilities\n",
        "  next_token_probs = torch.softmax(next_token_logits, -1)\n",
        "  # 3. step to get the top words - numOfWords is the amount\n",
        "  topk_next_tokens= torch.topk(next_token_probs, numOfWords)\n",
        "  #putting it together\n",
        "  dictionary = {tokenizer.decode(idx): prob for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)}\n",
        "  #We also need to clear the words from the symbols - for the proper comparing\n",
        "  clearedKeys = []\n",
        "  for item in dictionary.keys():\n",
        "    clearedKeys.append(ClearString(item))\n",
        "  finDictionary = dict(zip(clearedKeys, dictionary.values()))\n",
        "  return finDictionary"
      ],
      "metadata": {
        "id": "fYvaOaN1F2gc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def CodeAppending(words, wordsWithProbs, currentIndex, maxIndex, code):\n",
        "  wordCode = -1\n",
        "  #Is the next word from the sentence present in the dictionary\n",
        "  isWordPresentInVector = words[currentIndex] in wordsWithProbs.keys()\n",
        "  if isWordPresentInVector:\n",
        "    #Find the index of the element - it will become the code\n",
        "    for i, key in enumerate(wordsWithProbs.keys()):\n",
        "      if key == words[currentIndex]:\n",
        "        wordCode = i\n",
        "        break\n",
        "  #The words-numbers like 20 or -14.5 get put in the specific brackets and aquire the letter on the front\n",
        "  #The decoder will see this letter and know it's a word-number, not a word code\n",
        "  if words[currentIndex].isdigit():  #for integers\n",
        "    code.append(numberWordKeyS)\n",
        "    code.append(\"n\" + words[currentIndex])\n",
        "    code.append(numberWordKeyE)\n",
        "  elif re.match(r\"[-+]?(?:\\d*\\.*\\d+)\", words[currentIndex]):  #for floating-point numbers\n",
        "    code.append(numberWordKeyS)\n",
        "    code.append(\"n\" + words[currentIndex])\n",
        "    code.append(numberWordKeyE)\n",
        "  #If the code is less than the maximum allowed, put it in code\n",
        "  elif isWordPresentInVector and wordCode <= maxIndex and wordCode >= 0:\n",
        "    code.append(wordCode)\n",
        "  #If the word is present in the dictionary, but the code is greater than the maximum allowed\n",
        "  #The word is put into the code between the specific brackets\n",
        "  elif isWordPresentInVector and wordCode > maxIndex:\n",
        "    code.append(notFitKeyS)\n",
        "    code.append(words[currentIndex])\n",
        "    code.append(notFitKeyE)\n",
        "  #If the word didn't appear in the dictionary, it gets put in code between the corresponding brackets\n",
        "  else:\n",
        "    code.append(unknownKeyS)\n",
        "    code.append(words[currentIndex])\n",
        "    code.append(unknownKeyE)"
      ],
      "metadata": {
        "id": "QWyHHNAeGyIb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT2Tokenizer has a model trained for Ukrainian language. Link: https://huggingface.co/benjamin/gpt2-wechsel-ukrainian/blame/main/tokenizer_config.json\n",
        "tUkr = GPT2TokenizerFast.from_pretrained(\"benjamin/gpt2-wechsel-ukrainian\")\n",
        "mUkr = GPT2LMHeadModel.from_pretrained(\"benjamin/gpt2-wechsel-ukrainian\")\n",
        "#Test string\n",
        "encoded_text = tUkr(\"Мені все вдається легко, тому\", return_tensors=\"pt\")\n",
        "#The scheme is the same\n",
        "#1. step to get the logits of the next token\n",
        "with torch.inference_mode():\n",
        "  outputsUkr = mUkr(**encoded_text)\n",
        "next_token_logits = outputsUkr.logits[0, -1, :]\n",
        "print(next_token_logits.shape)\n",
        "# 2. step to convert the logits to probabilities\n",
        "next_token_probs = torch.softmax(next_token_logits, -1)\n",
        "# 3. step to get the top 20\n",
        "topk_next_tokens = torch.topk(next_token_probs, 20)\n",
        "#putting it together\n",
        "print(*[(tUkr.decode(idx), prob) for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)], sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQPPEhHGudIF",
        "outputId": "86c34c1c-99fb-4fe2-eedd-5b7b613638e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257])\n",
            "(' що', tensor(0.4398))\n",
            "(' я', tensor(0.0823))\n",
            "(',', tensor(0.0243))\n",
            "(' й', tensor(0.0208))\n",
            "(' не', tensor(0.0199))\n",
            "(' і', tensor(0.0193))\n",
            "(' ми', tensor(0.0110))\n",
            "(' рекомендую', tensor(0.0098))\n",
            "(' в', tensor(0.0086))\n",
            "(' раджу', tensor(0.0075))\n",
            "(' мені', tensor(0.0072))\n",
            "(' з', tensor(0.0065))\n",
            "(' ви', tensor(0.0046))\n",
            "(' це', tensor(0.0046))\n",
            "(' якщо', tensor(0.0045))\n",
            "(' у', tensor(0.0042))\n",
            "(' буду', tensor(0.0039))\n",
            "(' все', tensor(0.0037))\n",
            "(' як', tensor(0.0036))\n",
            "(' дякую', tensor(0.0036))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATE the text using the GPT-2 model\n",
        "#Link to code: https://medium.com/@majd.farah08/generating-text-with-gpt2-in-under-10-lines-of-code-5725a38ea685\n",
        "\n",
        "modelForGeneration = GPT2LMHeadModel.from_pretrained(\"benjamin/gpt2-wechsel-ukrainian\", pad_token_id=tUkr.eos_token_id)\n",
        "#The prompt for generation\n",
        "prompt = \"Навесні всі каштани вздовж нашої вулиці вкривалися білими та рожевими квітками\"\n",
        "#Encode the prompt\n",
        "encoded_text = tUkr.encode(prompt, return_tensors=\"pt\")\n",
        "#Put it into model to generate the text\n",
        "output = modelForGeneration.generate(encoded_text, max_length=300, do_sample=True, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "finalText = tUkr.decode(output[0], skip_special_tokens=True)\n",
        "#Print the text\n",
        "print(finalText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqySaucijrGK",
        "outputId": "75dff582-3e83-49bb-c8a6-2214efb0834a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Навесні всі каштани вздовж нашої вулиці вкривалися білими та рожевими квітками. Це було свято весни.\n",
            "На початку весни, коли природа прокидається від зимової сплячки, ми святкуємо День весняного рівнодення. Саме в цей день ми відзначаємо свято всіх закоханих, а також тих, хто любить і поважає один одного. Цей день є символом любові, вірності, ніжності та вірності. З давніх-давен наші предки вважали, що саме в цю пору року весна пробуджує природу від зимового сну і дарує надію на краще майбутнє. Наші пращури вірили, якщо на Вербну неділю не буде ясного дня, то зима буде холодною і сніжною. Тому, щоб цього дня не було холодно, в народі говорили: «Верба на вербі, верба в хаті».\n",
            "У народі існує повір'я: якщо в перший день весни погода була ясною і сонячною – весна буде довгою і щедрою на врожай. Якщо ж небо було похмурим і вкрите хмарами – літо буде дощовим і вітряним. А якщо день був ясним та безхмарним – осінь буде теплою і сухою. У народі кажуть: “Яка погода, така й зима”.\n",
            "В Україні з давніх давен шанували вербу, як символ дівочої краси, працьовитості, краси та кохання. Верба здавна використовувалась як оберіг від злих сил. До наших днів дійшла легенда, згідно з якою в давні часи дівчата, які ворожили на гіл\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive and input the .tsv file with the texts and their prompts\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "dataframe = pd.read_csv('/content/gdrive/My Drive/Kaggle/GPT-2_Data_From_Text_Generation.tsv', sep='\\t', header=None)\n",
        "dataframe.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Tr31uB90qaDQ",
        "outputId": "795484a6-ebd9-4b33-d85f-658d03c2d827"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0  \\\n",
              "0        Сонячний літній ліс, в якому я люблю гуляти   \n",
              "1        Сонячний літній ліс, в якому я люблю гуляти   \n",
              "2  Сьогодні почалася війна. Я прокинулася від зву...   \n",
              "3        Сонячний літній ліс, в якому я люблю гуляти   \n",
              "4        Сонячний літній ліс, в якому я люблю гуляти   \n",
              "\n",
              "                                                   1  \n",
              "0  Сонячний літній ліс, в якому я люблю гуляти, -...  \n",
              "1  Сонячний літній ліс, в якому я люблю гуляти, с...  \n",
              "2  Сьогодні почалася війна. Я прокинулася від зву...  \n",
              "3  Сонячний літній ліс, в якому я люблю гуляти. Т...  \n",
              "4  Сонячний літній ліс, в якому я люблю гуляти, –...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee196bf2-cc15-405e-a802-2d09fd11af88\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти</td>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти</td>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти, с...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Сьогодні почалася війна. Я прокинулася від зву...</td>\n",
              "      <td>Сьогодні почалася війна. Я прокинулася від зву...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти</td>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти. Т...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти</td>\n",
              "      <td>Сонячний літній ліс, в якому я люблю гуляти, –...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee196bf2-cc15-405e-a802-2d09fd11af88')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee196bf2-cc15-405e-a802-2d09fd11af88 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee196bf2-cc15-405e-a802-2d09fd11af88');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-49a1d445-b993-4a82-9a33-9f75e6a788a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49a1d445-b993-4a82-9a33-9f75e6a788a3')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-49a1d445-b993-4a82-9a33-9f75e6a788a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the, for example, last element of the dataframe\n",
        "#This element will be examined in detail\n",
        "generatedText = dataframe[1].iloc[-1]\n",
        "print(generatedText)\n",
        "#All the other elements will be examined in the end in a loop of measure-encode-compress-measure-decode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igj_0YLo1EL6",
        "outputId": "78ad7ba0-2996-43ee-a7ba-a88da5142b82"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сонячний літній ліс, в якому я люблю гуляти, – це місце, куди хочеться повертатися знову і знову. Я ніколи не забуду цього чарівного місця, де я народилася і виросла. Тут я знайшла себе, тут я зустріла свою другу половинку. Я завжди мріяла про те, щоб у мене була своя сім’я. Але коли я стала дорослою, я зрозуміла, що це не так просто. І мені захотілося зробити щось більше, ніж просто бути мамою. У мене з’явилося багато друзів, з якими я можу поділитися своїми захопленнями, своїми думками і переживаннями. Мені дуже подобається подорожувати, і я дуже рада, коли мої друзі приїжджають до мене в гості. Коли я приїжджаю до них, мені хочеться поділитися з ними своєю любов’ю і турботою про мене. Тому я завжди буду рада знайомству з новими людьми, які захочуть поділитися своїм теплом і ніжністю з моїми близькими людьми. Це дуже важливо для мене, тому що я відчуваю, як мої близькі люди відчувають мою любов і турботу. А це дуже приємно для нас обох. Адже ми любимо один одного і дуже хочемо бути разом. Якщо у вас є діти, ви завжди можете їм допомогти, якщо у них є проблеми зі здоров’ям, вони завжди можуть прийти до вас на прийом і поговорити з вами. Ви завжди зможете знайти спільну мову з дітьми і розповісти про свої проблеми і проблеми своїх близьких людей. Так само ви можете запросити їх до себе на відпочинок або в школу.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "promptSizeUkr = 15  #The maximum length of a prompt\n",
        "maxIndexUkr = 248  #Max code to insert in the code list\n",
        "unknownKeyS = 254\n",
        "unknownKeyE = 255  #If the word doesn't appear in the tensor - we put it in the code, bracketing it with these two keys\n",
        "notFitKeyS = 252\n",
        "notFitKeyE = 253  #If the word appeared in tensor, but didn't fit within the code limit - we put it in code, bracketing with these two keys\n",
        "numberWordKeyS = 250\n",
        "numberWordKeyE = 251  #If the word is a number and doesn't have to be decrypted while decoding, we put it in code, bracketing it with these two keys\n",
        "endOfPromptKey = 249  #This key is put in code after the prompt is put there\n",
        "\n",
        "#The text made by ChatGPT 3.5\n",
        "textG35 = \"Сонячний ліс влітку був найкращим місцем для відпочинку. Під високими кронами дерев розквітали барвисті квіти, а гілки м'яко похитувалися на вітрі. Коли я вирушив на свою прогулянку, сонце щедро розпростерло свої промені, створюючи мерехтливі плями на стежці. Початкові кроки привели мене до гаю з величезними дубами і кленами. Легкий шум листя надавав лісу власний ритм, а спів пташок створював мелодійну симфонію. Змінюючи напрямок руху, я опинився біля маленького струмка. Вода пливла спокійно і чисто, а на його березі росли жовті лілії, що розцвітали в воді. Пройшовши глибше в ліс, я натрапив на лісовий майданчик, де сонячні промені змішувалися з тінями дерев. Тут я вирішив влаштувати пікнік, розклавши ковдру і розмістивши корзину з їжею. Затишна атмосфера і шум природи зробили мій обід надзвичайно приємним. Найбільше мене вразила зустріч з дикою природою. Барвисті метелики летіли навколо мене, а білки стежили за моїми рухами. Поруч із струмком, я випив прохолодної води і відчував себе щасливим. Сонце спускалося за горизонт, і коли я повертався додому, усе ще чув спів пташок у лісі. Та прогулянка лісом влітку залишила в мене незабутні враження і нагадала, наскільки важливо дбати про нашу природу і насолоджуватися її красою.\"\n",
        "wordsG35 = ClearElements(textG35.split())  #Split the text by spaces and clear it from delimiters and any symbols except for words themselves\n",
        "promptG35 = wordsG35[:promptSizeUkr]  #The beginning of the text to keep safe for decoding afterwards\n",
        "#The key - starts as a prompt, but transforms after each word encoded\n",
        "keyG35 = wordsG35[:promptSizeUkr]\n",
        "#The encoded text - prompt is put here, and all other codes are too\n",
        "codeG35 = wordsG35[:promptSizeUkr]\n",
        "codeG35.append(endOfPromptKey)\n",
        "currentIndexG35 = promptSizeUkr + 1\n",
        "\n",
        "textG2 = generatedText\n",
        "print(textG2)\n",
        "wordsG2 = ClearElements(textG2.split())  #The same inits for the text generated by ChatGPT 2\n",
        "promptG2 = wordsG2[:promptSizeUkr]\n",
        "keyG2 = wordsG2[:promptSizeUkr]\n",
        "codeG2 = wordsG2[:promptSizeUkr]\n",
        "codeG2.append(endOfPromptKey)\n",
        "currentIndexG2 = promptSizeUkr + 1\n",
        "#Human-written text\n",
        "#Link to the text: https://glazastik.com/%d0%bb%d1%96%d1%81-%d0%b2%d0%bb%d1%96%d1%82%d0%ba%d1%83-%d1%82%d0%b2%d1%96%d1%80-%d0%be%d0%bf%d0%b8%d1%81-%d1%82%d0%b5%d0%ba%d1%81%d1%82-%d1%80%d0%be%d0%b7%d0%bf%d0%be%d0%b2%d1%96%d0%b4%d1%8c/\n",
        "textH = \"Ліс прекрасний у будь-який час року, але особливо влітку. Коли заходиш у нього, то відразу ж змінюється настрій, стає легше дихати. У спекотну погоду відчувається приємна прохолода. В холодний день, навпаки, нагріті стовбури дерев віддають своє тепло. При вході відразу ж уся увага звертається на дерева. У змішаному лісі можна побачити величні дуби, стрункі сосни та пірамідальні ялини. Дуже гарно виглядають білокорі берези, вони завжди радісно шелестять листячком на вітрі. Під деревами навкруги ростуть чагарники. Всі рослини тягнуться вгору, ближче до світла й сонця. Ліс влітку дуже багатий різноманітною рослинністю. Всюди висока трава. Особливо приємно знайти сонячну галявину, яка вся в квітах конюшини, ромашки, гвоздики, кашок. Багато квітучих лікарських рослин: звіробій, материнка, деревій. А можна й наштовхнутися на таке місце, де навколо червоніють ягоди суниці. Потрібно тільки не лінуватися, щоб її збирати. У густих заростях зустрічається чорниця. Опівдні температура повітря стає високою, квітучі трави починають пахнути. Дуже приємно лягти серед цих квітів та вдихати аромати квітів. А ще — дивитися, як по блакитному небу пропливають хмари та вгадувати, на що вони схожі. Отже, ліс влітку — осередок краси та єднання з природою. Обов’язково відвідайте це зелене царство!\"\n",
        "\n",
        "wordsH = ClearElements(textH.split())  #The same inits for the human-written text\n",
        "promptH = wordsH[:promptSizeUkr]\n",
        "keyH = wordsH[:promptSizeUkr]\n",
        "codeH = wordsH[:promptSizeUkr]\n",
        "codeH.append(endOfPromptKey)\n",
        "currentIndexH = promptSizeUkr + 1\n",
        "\n",
        "print(promptG35)\n",
        "print(promptG2)\n",
        "print(promptH)\n",
        "print(len(wordsG35))\n",
        "print(len(textG2.split()))\n",
        "print(len(textH.split()))\n",
        "print(len(textG35))\n",
        "print(len(textG2))\n",
        "print(len(textH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR_4D7qa3ckK",
        "outputId": "517891a1-8700-4a32-eabb-36c601d78cfa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сонячний літній ліс, в якому я люблю гуляти, – це місце, куди хочеться повертатися знову і знову. Я ніколи не забуду цього чарівного місця, де я народилася і виросла. Тут я знайшла себе, тут я зустріла свою другу половинку. Я завжди мріяла про те, щоб у мене була своя сім’я. Але коли я стала дорослою, я зрозуміла, що це не так просто. І мені захотілося зробити щось більше, ніж просто бути мамою. У мене з’явилося багато друзів, з якими я можу поділитися своїми захопленнями, своїми думками і переживаннями. Мені дуже подобається подорожувати, і я дуже рада, коли мої друзі приїжджають до мене в гості. Коли я приїжджаю до них, мені хочеться поділитися з ними своєю любов’ю і турботою про мене. Тому я завжди буду рада знайомству з новими людьми, які захочуть поділитися своїм теплом і ніжністю з моїми близькими людьми. Це дуже важливо для мене, тому що я відчуваю, як мої близькі люди відчувають мою любов і турботу. А це дуже приємно для нас обох. Адже ми любимо один одного і дуже хочемо бути разом. Якщо у вас є діти, ви завжди можете їм допомогти, якщо у них є проблеми зі здоров’ям, вони завжди можуть прийти до вас на прийом і поговорити з вами. Ви завжди зможете знайти спільну мову з дітьми і розповісти про свої проблеми і проблеми своїх близьких людей. Так само ви можете запросити їх до себе на відпочинок або в школу.\n",
            "['Сонячний', 'ліс', 'влітку', 'був', 'найкращим', 'місцем', 'для', 'відпочинку', 'Під', 'високими', 'кронами', 'дерев', 'розквітали', 'барвисті', 'квіти']\n",
            "['Сонячний', 'літній', 'ліс', 'в', 'якому', 'я', 'люблю', 'гуляти', '–', 'це', 'місце', 'куди', 'хочеться', 'повертатися', 'знову']\n",
            "['Ліс', 'прекрасний', 'у', 'будь-який', 'час', 'року', 'але', 'особливо', 'влітку', 'Коли', 'заходиш', 'у', 'нього', 'то', 'відразу']\n",
            "189\n",
            "235\n",
            "187\n",
            "1242\n",
            "1333\n",
            "1292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Before encoding-compression-decoding, analyze the texts\n",
        "import bz2\n",
        "\n",
        "print(\"ENTROPY\")\n",
        "#The higher Entropy means the text is less likely to be artificial\n",
        "print(\"ChatGPT 3.5: \" + str(ShannonEntropy(textG35)))  #Generated text (ChatGPT 3.5)\n",
        "print(\"ChatGPT 2: \" + str(ShannonEntropy(textG2)))  #Generated text (ChatGPT 2)\n",
        "print(\"Human: \" + str(ShannonEntropy(textH)))  #Seemingly human text\n",
        "\n",
        "print(\"AVERAGE\")\n",
        "#As the English language model works with Ukrainian sentences, it is used here\n",
        "procGenerText = engnlp(textG35)\n",
        "procG2Text = engnlp(textG2)\n",
        "procHumanText = engnlp(textH)\n",
        "sumG = 0\n",
        "for phrase in procGenerText._.phrases:\n",
        "    sumG = sumG + phrase.rank\n",
        "sumG2 = 0\n",
        "for phrase in procG2Text._.phrases:\n",
        "    sumG2 = sumG2 + phrase.rank\n",
        "sumH = 0\n",
        "for phrase in procHumanText._.phrases:\n",
        "    sumH = sumH + phrase.rank\n",
        "#Phrase rank Average. The higher Average means the text is less likely to be artificial\n",
        "averageG = sumG / len(procGenerText._.phrases)\n",
        "print(\"ChatGPT 3.5: \" + str(averageG))\n",
        "averageG2 = sumG2 / len(procG2Text._.phrases)\n",
        "print(\"ChatGPT 2: \" + str(averageG2))\n",
        "averageH = sumH / len(procHumanText._.phrases)\n",
        "print(\"Human: \" + str(averageH))\n",
        "\n",
        "print(\"VARIANCE\")\n",
        "listG = []\n",
        "for phrase in procGenerText._.phrases:\n",
        "    listG.append(phrase.rank)\n",
        "listG2 = []\n",
        "for phrase in procG2Text._.phrases:\n",
        "    listG2.append(phrase.rank)\n",
        "listH = []\n",
        "for phrase in procHumanText._.phrases:\n",
        "    listH.append(phrase.rank)\n",
        "\n",
        "#The lower Variance means the text is less likely to be edited\n",
        "print(\"ChatGPT 3.5: \" + str(statistics.variance(listG)))\n",
        "print(\"ChatGPT 2: \" + str(statistics.variance(listG2)))\n",
        "print(\"Human: \" + str(statistics.variance(listH)))\n",
        "\n",
        "print(\"COMPRESSION RATE (ORIGINAL)\")\n",
        "#The better the text is compressed, the more artificial it may be\n",
        "compressedG = bz2.compress(textG35.encode())\n",
        "compressedH = bz2.compress(textH.encode())\n",
        "decompressedG = bz2.decompress(compressedG).decode()\n",
        "decompressedH = bz2.decompress(compressedH).decode()\n",
        "compressedG2 = bz2.compress(textG2.encode())\n",
        "decompressedG2 = bz2.decompress(compressedG2).decode()\n",
        "print(\"ChatGPT 3.5: \" + str(len(textG35)/len(compressedG)))\n",
        "print(\"ChatGPT 2: \" + str(len(textG2)/len(compressedG2)))\n",
        "print(\"Human: \" + str(len(textH)/len(compressedH)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IVgzEFsisI8",
        "outputId": "db50f0a5-60fe-4a16-85af-e94e03501fb6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTROPY\n",
            "ChatGPT 3.5: 4.595582247453061\n",
            "ChatGPT 2: 4.614975892208419\n",
            "Human: 4.695354496057342\n",
            "AVERAGE\n",
            "ChatGPT 3.5: 0.05771758587525143\n",
            "ChatGPT 2: 0.06091264171128362\n",
            "Human: 0.04519870368921074\n",
            "VARIANCE\n",
            "ChatGPT 3.5: 0.0004942431435823742\n",
            "ChatGPT 2: 0.000819278385091761\n",
            "Human: 0.0004650122788464069\n",
            "COMPRESSION RATE (ORIGINAL)\n",
            "ChatGPT 3.5: 1.6108949416342413\n",
            "ChatGPT 2: 1.706786171574904\n",
            "Human: 1.5736906211936663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IF NEEDED, it works\n",
        "#ENCODE the prompt\n",
        "#In the code, only the first word will be written, other ones will be coded\n",
        "\n",
        "def PromptEncode(code, key, numberOfWords, prompt, model, tokenizer, maxIndex):\n",
        "  counter = 0\n",
        "  for word in prompt:\n",
        "    if counter == 0:  #The first word must be put into code\n",
        "      key.append(word)\n",
        "      code.append(word)\n",
        "      counter = counter + 1\n",
        "    else:\n",
        "      #Combine the words already present in key to make the string\n",
        "      string = CombineWordsInString(key)\n",
        "      #Encode the string\n",
        "      encoded_text = tokenizer(string, return_tensors=\"pt\")\n",
        "      #Get the possible next words\n",
        "      wordsWithProbs = FindWordsDictionary(encoded_text, maxIndex * 2, model, tokenizer)\n",
        "      #Append the encoded word to the code\n",
        "      CodeAppending(prompt, wordsWithProbs, counter, maxIndex, code)\n",
        "      key.append(word)  #And put the original word into the key\n",
        "      counter = counter + 1\n",
        "  code.append(endOfPromptKey)"
      ],
      "metadata": {
        "id": "5D3G9j4LGBJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IF NEEDED, it works\n",
        "PromptEncode(codeG35, keyG35, promptSizeUkr, promptG35, mUkr, tUkr, maxIndexUkr)\n",
        "print(promptG35)\n",
        "print(keyG35)\n",
        "print(codeG35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0Vb9DAzI3MH",
        "outputId": "afe13a35-a9c3-47d8-f05a-f1c3ce4ed3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['І', 'ось', 'ми', 'тут', 'робимо', 'лабораторну', 'роботу']\n",
            "['І', 'ось', 'ми', 'тут', 'робимо', 'лабораторну', 'роботу']\n",
            "['І', 254, 'ось', 255, 3, 23, 95, 254, 'лабораторну', 255, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCODE first\n",
        "def Encode(currentIndex, words, key, tokenizer, model, maxIndex, code):\n",
        "  while currentIndex < len(words):\n",
        "    #The key is kept in separate words. For encoding, they must be put into one string\n",
        "    stringUkr = CombineWordsInString(key)\n",
        "    #Encode the string\n",
        "    encoded_text = tokenizer(stringUkr, return_tensors=\"pt\")\n",
        "    #Get words with the highest probabilities to appear after the string\n",
        "    wordsWithProbsUkr = FindWordsDictionary(encoded_text, maxIndexUkr + 1, model, tokenizer)\n",
        "    #Append the code of the word (or the word itself) to the code\n",
        "    CodeAppending(words, wordsWithProbsUkr, currentIndex, maxIndex, code)\n",
        "    #Remove the first element from the key\n",
        "    key.pop(0)\n",
        "    #Add the new word to the key\n",
        "    key.append(words[currentIndex])\n",
        "    #Increase the currentIndex\n",
        "    currentIndex = currentIndex + 1"
      ],
      "metadata": {
        "id": "KDdzQjhg4NSQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encode(currentIndexG35, wordsG35, keyG35, tUkr, mUkr, maxIndexUkr, codeG35)\n",
        "Encode(currentIndexG2, wordsG2, keyG2, tUkr, mUkr, maxIndexUkr, codeG2)\n",
        "Encode(currentIndexH, wordsH, keyH, tUkr, mUkr, maxIndexUkr, codeH)\n",
        "\n",
        "print(codeG35)\n",
        "print(codeG2)\n",
        "print(codeH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srd9Z5ebdeq3",
        "outputId": "b1a740fc-8a7d-47e9-d9d5-20ca5866a6f5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Сонячний', 'ліс', 'влітку', 'був', 'найкращим', 'місцем', 'для', 'відпочинку', 'Під', 'високими', 'кронами', 'дерев', 'розквітали', 'барвисті', 'квіти', 249, 254, 'гілки', 255, 254, \"м'яко\", 255, 254, 'похитувалися', 255, 3, 254, 'вітрі', 255, 254, 'Коли', 255, 7, 254, 'вирушив', 255, 2, 6, 42, 254, 'сонце', 255, 254, 'щедро', 255, 254, 'розпростерло', 255, 0, 0, 254, 'створюючи', 255, 254, 'мерехтливі', 255, 6, 0, 254, 'стежці', 255, 254, 'Початкові', 255, 9, 25, 2, 0, 254, 'гаю', 255, 4, 254, 'величезними', 255, 254, 'дубами', 255, 2, 254, 'кленами', 255, 254, 'Легкий', 255, 69, 30, 107, 26, 254, 'власний', 255, 34, 63, 39, 254, 'пташок', 255, 254, 'створював', 255, 254, 'мелодійну', 255, 254, 'симфонію', 255, 254, 'Змінюючи', 255, 33, 2, 40, 254, 'опинився', 255, 3, 37, 254, 'струмка', 255, 20, 254, 'пливла', 255, 139, 1, 100, 87, 7, 4, 2, 49, 48, 254, 'лілії', 255, 7, 254, 'розцвітали', 255, 3, 254, 'воді', 255, 254, 'Пройшовши', 255, 235, 3, 4, 2, 254, 'натрапив', 255, 0, 254, 'лісовий', 255, 254, 'майданчик', 255, 3, 254, 'сонячні', 255, 0, 254, 'змішувалися', 255, 0, 254, 'тінями', 255, 4, 254, 'Тут', 255, 0, 16, 34, 254, 'пікнік', 255, 254, 'розклавши', 255, 254, 'ковдру', 255, 1, 254, 'розмістивши', 255, 254, 'корзину', 255, 0, 2, 254, 'Затишна', 255, 0, 2, 205, 3, 24, 254, 'мій', 255, 6, 34, 2, 254, 'Найбільше', 255, 5, 2, 174, 0, 254, 'дикою', 255, 0, 254, 'Барвисті', 255, 254, 'метелики', 255, 254, 'летіли', 255, 17, 0, 32, 254, 'білки', 255, 254, 'стежили', 255, 0, 4, 0, 254, 'Поруч', 255, 1, 254, 'струмком', 255, 1, 254, 'випив', 255, 254, 'прохолодної', 255, 0, 2, 208, 1, 15, 254, 'Сонце', 255, 254, 'спускалося', 255, 16, 1, 2, 77, 0, 17, 1, 254, 'усе', 255, 1, 96, 9, 254, 'пташок', 255, 4, 2, 254, 'Та', 255, 254, 'прогулянка', 255, 4, 148, 40, 1, 7, 1, 0, 4, 254, 'нагадала', 255, 254, 'наскільки', 255, 0, 20, 0, 61, 0, 2, 144, 1, 0]\n",
            "['Сонячний', 'літній', 'ліс', 'в', 'якому', 'я', 'люблю', 'гуляти', '–', 'це', 'місце', 'куди', 'хочеться', 'повертатися', 'знову', 249, 11, 254, 'Я', 255, 19, 0, 254, 'забуду', 255, 9, 254, 'чарівного', 255, 1, 5, 0, 1, 1, 1, 28, 0, 1, 2, 11, 0, 1, 0, 1, 254, 'половинку', 255, 1, 20, 254, 'мріяла', 255, 0, 0, 1, 1, 0, 0, 2, 254, 'сім’я', 255, 27, 3, 0, 3, 254, 'дорослою', 255, 1, 0, 1, 0, 0, 0, 2, 77, 3, 254, 'захотілося', 255, 0, 0, 10, 1, 0, 40, 1, 127, 0, 254, 'з’явилося', 255, 0, 0, 7, 0, 0, 1, 0, 0, 254, 'захопленнями', 255, 31, 4, 1, 254, 'переживаннями', 255, 254, 'Мені', 255, 1, 0, 6, 2, 1, 2, 2, 9, 0, 0, 8, 1, 0, 0, 0, 254, 'Коли', 255, 0, 254, 'приїжджаю', 255, 0, 0, 17, 4, 10, 0, 0, 2, 254, 'любов’ю', 255, 2, 254, 'турботою', 255, 2, 1, 254, 'Тому', 255, 0, 2, 1, 0, 254, 'знайомству', 255, 0, 9, 0, 40, 254, 'захочуть', 255, 1, 1, 6, 0, 254, 'ніжністю', 255, 1, 10, 5, 1, 254, 'Це', 255, 1, 0, 1, 0, 31, 1, 0, 2, 13, 2, 0, 0, 1, 3, 1, 0, 1, 254, 'А', 255, 15, 1, 2, 5, 2, 3, 12, 0, 0, 0, 0, 0, 8, 2, 1, 0, 254, 'Якщо', 255, 1, 1, 0, 0, 18, 0, 0, 13, 0, 6, 0, 1, 0, 0, 1, 254, 'здоров’ям', 255, 9, 0, 0, 4, 1, 1, 0, 1, 4, 8, 0, 1, 254, 'Ви', 255, 2, 1, 0, 0, 0, 0, 10, 2, 20, 0, 0, 5, 2, 1, 1, 1, 3, 254, 'Так', 255, 2, 7, 0, 25, 2, 1, 0, 1, 8, 2, 1, 20]\n",
            "['Ліс', 'прекрасний', 'у', 'будь-який', 'час', 'року', 'але', 'особливо', 'влітку', 'Коли', 'заходиш', 'у', 'нього', 'то', 'відразу', 249, 198, 0, 81, 28, 16, 67, 254, 'спекотну', 255, 0, 123, 23, 254, 'прохолода', 255, 10, 26, 2, 50, 254, 'нагріті', 255, 254, 'стовбури', 255, 0, 133, 12, 0, 63, 254, 'вході', 255, 254, 'відразу', 255, 0, 254, 'уся', 255, 21, 0, 0, 54, 29, 254, 'змішаному', 255, 1, 2, 0, 254, 'величні', 255, 0, 254, 'стрункі', 255, 22, 4, 254, 'пірамідальні', 255, 254, 'ялини', 255, 90, 0, 0, 254, 'білокорі', 255, 254, 'берези', 255, 254, 'вони', 255, 16, 254, 'радісно', 255, 254, 'шелестять', 255, 254, 'листячком', 255, 5, 254, 'вітрі', 255, 67, 13, 254, 'навкруги', 255, 7, 254, 'чагарники', 255, 254, 'Всі', 255, 5, 254, 'тягнуться', 255, 2, 254, 'ближче', 255, 0, 9, 21, 2, 157, 254, 'влітку', 255, 5, 4, 254, 'різноманітною', 255, 254, 'рослинністю', 255, 254, 'Всюди', 255, 254, 'висока', 255, 0, 189, 99, 218, 254, 'сонячну', 255, 254, 'галявину', 255, 118, 254, 'вся', 255, 0, 254, 'квітах', 255, 254, 'конюшини', 255, 35, 254, 'гвоздики', 255, 254, 'кашок', 255, 254, 'Багато', 255, 254, 'квітучих', 255, 72, 0, 254, 'звіробій', 255, 254, 'материнка', 255, 254, 'деревій', 255, 254, 'А', 255, 254, 'можна', 255, 27, 254, 'наштовхнутися', 255, 0, 11, 16, 5, 254, 'навколо', 255, 254, 'червоніють', 255, 8, 254, 'суниці', 255, 254, 'Потрібно', 255, 6, 0, 254, 'лінуватися', 255, 69, 10, 53, 28, 254, 'густих', 255, 254, 'заростях', 255, 254, 'зустрічається', 255, 254, 'чорниця', 255, 254, 'Опівдні', 255, 254, 'температура', 255, 0, 25, 25, 254, 'квітучі', 255, 22, 7, 254, 'пахнути', 255, 254, 'Дуже', 255, 18, 254, 'лягти', 255, 37, 9, 0, 5, 254, 'вдихати', 255, 4, 2, 18, 0, 38, 21, 2, 82, 254, 'блакитному', 255, 0, 254, 'пропливають', 255, 0, 3, 254, 'вгадувати', 255, 8, 1, 0, 0, 254, 'Отже', 255, 254, 'ліс', 255, 254, 'влітку', 255, 26, 254, 'осередок', 255, 107, 1, 94, 1, 0, 254, 'Обов’язково', 255, 254, 'відвідайте', 255, 3, 34, 254, 'царство!', 255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IF NEEDED, it works\n",
        "#The prompt must also be DECODED\n",
        "#This variable is the amount of words / codes to skip from the beginning when the text decoding will start\n",
        "decodeIndexUkr = 0\n",
        "\n",
        "def PromptDecode(encodedWords, decodedText, decodedKey, model, tokenizer, maxIndex, promptLength, input):\n",
        "  global decodeIndexUkr\n",
        "  counter = 0\n",
        "  while 1:\n",
        "    word = encodedWords[decodeIndexUkr]\n",
        "    #The first word is taken directly\n",
        "    if decodeIndexUkr == 0:\n",
        "      decodedText.append(word)\n",
        "      decodedKey.append(word)\n",
        "      counter = counter + 1\n",
        "      decodeIndexUkr = decodeIndexUkr + 1\n",
        "    else:\n",
        "      #If the end of prompt is reached, break the loop\n",
        "      if word == endOfPromptKey:\n",
        "        decodeIndexUkr = decodeIndexUkr + 1\n",
        "        break\n",
        "      #If a bracket is detected, check if, without the first letter, the word between the brackets is a number\n",
        "      if word == unknownKeyS or word == notFitKeyS or word == numberWordKeyS:\n",
        "        tempWord = encodedWords[decodeIndexUkr + 1].replace(encodedWords[decodeIndexUkr + 1][0], \"\", 1)\n",
        "        if tempWord.isdigit():  #If it is, put the number into the decrypted text\n",
        "          decodedText.append(tempWord)\n",
        "          decodedKey.append(tempWord)\n",
        "        else:  #If it is not, put the word into the decrypted text without changes\n",
        "          decodedText.append(encodedWords[decodeIndexUkr + 1])\n",
        "          decodedKey.append(encodedWords[decodeIndexUkr + 1])\n",
        "        decodeIndexUkr = decodeIndexUkr + 3  #The index is increased by 3 - two brackets and the word\n",
        "        counter = counter + 1  #Word counter is increased by 1\n",
        "      #Otherwise, decode the word\n",
        "      else:\n",
        "        #Get the key as one string\n",
        "        stringKey = CombineWordsInString(decodedKey)\n",
        "        #Encode it\n",
        "        encoded_stringUkr = tokenizer(stringKey, return_tensors=\"pt\")\n",
        "        #Get possible next words\n",
        "        possiblewordsG35 = FindWordsDictionary(encoded_stringUkr, maxIndex * 2, model, tokenizer)\n",
        "        indexUkr = input[decodeIndexUkr]\n",
        "        word = \"\"\n",
        "        #Find the word on a position set by the code\n",
        "        for i, key in enumerate(possiblewordsG35.keys()):\n",
        "          if i == indexUkr:\n",
        "            word = key\n",
        "            break\n",
        "        #Append the word to the text and the key\n",
        "        decodedText.append(word)\n",
        "        decodeKey.append(word)\n",
        "        decodeIndexUkr = decodeIndexUkr + 1\n",
        "        counter = counter + 1"
      ],
      "metadata": {
        "id": "7jrDhsPIJLAG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StringsToIntsInACode(codeParts):\n",
        "  counter = 0\n",
        "  for codePart in codeParts:\n",
        "    if codePart.isdigit():\n",
        "      codeParts[counter] = int(codePart)\n",
        "    counter = counter + 1\n",
        "  return codeParts"
      ],
      "metadata": {
        "id": "-La3nU3Z413M"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now COMPRESS the code and DECOMPRESS it\n",
        "\n",
        "stringToCompressG35 = CombineWordsInString(codeG35)\n",
        "compressedG35 = bz2.compress(stringToCompressG35.encode())\n",
        "print(len(compressedG35))\n",
        "stringToCompressG2 = CombineWordsInString(codeG2)\n",
        "compressedG2 = bz2.compress(stringToCompressG2.encode())\n",
        "print(len(compressedG2))\n",
        "stringToCompressH = CombineWordsInString(codeH)\n",
        "compressedH = bz2.compress(stringToCompressH.encode())\n",
        "print(len(compressedH))\n",
        "\n",
        "decompressedG35 = bz2.decompress(compressedG35).decode()\n",
        "print(len(decompressedG35))\n",
        "codeMadeG35 = decompressedG35.split()\n",
        "print(codeMadeG35)\n",
        "decompressedG2 = bz2.decompress(compressedG2).decode()\n",
        "print(len(decompressedG2))\n",
        "codeMadeG2 = decompressedG2.split()\n",
        "print(codeMadeG2)\n",
        "decompressedH = bz2.decompress(compressedH).decode()\n",
        "print(len(decompressedH))\n",
        "codeMadeH = decompressedH.split()\n",
        "print(codeMadeH)\n",
        "\n",
        "codeMadeG35 = StringsToIntsInACode(codeMadeG35)\n",
        "codeMadeG2 = StringsToIntsInACode(codeMadeG2)\n",
        "codeMadeH = StringsToIntsInACode(codeMadeH)\n",
        "print(codeMadeG35)\n",
        "print(codeMadeG2)\n",
        "print(codeMadeH)\n",
        "\n",
        "print(\"COMPRESSION RATE (ENCODED)\")\n",
        "print(\"ChatGPT 3.5: \" + str(len(stringToCompressG35)/len(compressedG)))\n",
        "print(\"ChatGPT 2: \" + str(len(stringToCompressG2)/len(compressedG2)))\n",
        "print(\"Human: \" + str(len(stringToCompressH)/len(compressedH)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMYsx5mmg77C",
        "outputId": "d68fb382-f377-4e3a-9ec0-3b8930de5935"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137\n",
            "133\n",
            "125\n",
            "112\n",
            "['Сонячний', 'ліс', 'влітку', 'був', 'найкращим', 'місцем', 'для', 'відпочинку', 'Під', 'високими', 'кронами', 'дерев', 'розквітали', 'барвисті', 'квіти', '249']\n",
            "89\n",
            "['Сонячний', 'літній', 'ліс', 'в', 'якому', 'я', 'люблю', 'гуляти', '–', 'це', 'місце', 'куди', 'хочеться', 'повертатися', 'знову', '249']\n",
            "91\n",
            "['Ліс', 'прекрасний', 'у', 'будь-який', 'час', 'року', 'але', 'особливо', 'влітку', 'Коли', 'заходиш', 'у', 'нього', 'то', 'відразу', '249']\n",
            "['Сонячний', 'ліс', 'влітку', 'був', 'найкращим', 'місцем', 'для', 'відпочинку', 'Під', 'високими', 'кронами', 'дерев', 'розквітали', 'барвисті', 'квіти', 249]\n",
            "['Сонячний', 'літній', 'ліс', 'в', 'якому', 'я', 'люблю', 'гуляти', '–', 'це', 'місце', 'куди', 'хочеться', 'повертатися', 'знову', 249]\n",
            "['Ліс', 'прекрасний', 'у', 'будь-який', 'час', 'року', 'але', 'особливо', 'влітку', 'Коли', 'заходиш', 'у', 'нього', 'то', 'відразу', 249]\n",
            "COMPRESSION RATE (ENCODED)\n",
            "ChatGPT 3.5: 0.14526588845654995\n",
            "ChatGPT 2: 0.6691729323308271\n",
            "Human: 0.728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputG35 = codeG35[:]\n",
        "textG35 = []\n",
        "decodekeyG35 = []\n",
        "inputG2 = codeG2[:]\n",
        "textG2 = []\n",
        "decodekeyG2 = []\n",
        "inputH = codeH[:]\n",
        "textH = []\n",
        "decodekeyH = []\n",
        "\n",
        "#PromptDecode(inputUkr, textG35, decodekeyG35, mUkr, tUkr, maxIndexUkr, promptSizeUkr)\n",
        "decodeIndexG35 = 0\n",
        "print(textG35)\n",
        "print(decodekeyG35)\n",
        "print(decodeIndexG35)\n",
        "decodeIndexG2 = 0\n",
        "print(textG2)\n",
        "print(decodekeyG2)\n",
        "print(decodeIndexG2)\n",
        "decodeIndexH = 0\n",
        "print(textH)\n",
        "print(decodekeyH)\n",
        "print(decodeIndexH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgkH5frHNsRW",
        "outputId": "b2ca3a4a-9041-427f-b3e9-af0af0e707fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n",
            "0\n",
            "[]\n",
            "[]\n",
            "0\n",
            "[]\n",
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DECODE the text\n",
        "#The decoding goes through the same process as encoding\n",
        "def Decode(decodeIndex, code, decodeKey, text, input):\n",
        "  endOfPromptReached = False\n",
        "  while decodeIndex < len(code):\n",
        "    #If the word is the code of the end of prompt, don't put it in code, but change the endOfPromptReached variable\n",
        "    if input[decodeIndex] == endOfPromptKey:\n",
        "      decodeIndex = decodeIndex + 1\n",
        "      endOfPromptReached = True\n",
        "    #If the end of prompt is not reached, put the word unchanged\n",
        "    elif not endOfPromptReached:\n",
        "      text.append(code[decodeIndex])\n",
        "      decodeKey.append(code[decodeIndex])\n",
        "      decodeIndex = decodeIndex + 1\n",
        "    #If the code is one of the brackets, put the next word unchanged...\n",
        "    elif code[decodeIndex] == unknownKeyS or code[decodeIndex] == notFitKeyS or code[decodeIndex] == numberWordKeyS:\n",
        "      tempWordUkr = code[decodeIndex + 1].replace(code[decodeIndex + 1][0], \"\", 1)\n",
        "      #...Except if it's the number\n",
        "      if tempWordUkr.isdigit() or re.match(r\"[-+]?(?:\\d*\\.*\\d+)\", tempWordUkr):\n",
        "        text.append(tempWordUkr)\n",
        "        decodeKey.append(tempWordUkr)\n",
        "      else:\n",
        "        text.append(code[decodeIndex + 1])\n",
        "        decodeKey.append(code[decodeIndex + 1])\n",
        "      #First element is removed from the key, as the new one is placed into it\n",
        "      decodeKey.pop(0)\n",
        "      decodeIndex = decodeIndex + 3\n",
        "    #If the code is not the bracket, it must be decoded\n",
        "    else:\n",
        "      #Make the string from the key\n",
        "      stringkey = CombineWordsInString(decodeKey)\n",
        "      #Encode\n",
        "      encoded_stringUkr = tUkr(stringkey, return_tensors=\"pt\")\n",
        "      #Get possible words\n",
        "      possiblewords = FindWordsDictionary(encoded_stringUkr, maxIndexUkr + 1, mUkr, tUkr)\n",
        "      indexUkr = input[decodeIndex]\n",
        "      word = \"\"\n",
        "      #Find index of the word needed\n",
        "      for i, key in enumerate(possiblewords.keys()):\n",
        "        if i == indexUkr:\n",
        "          word = key\n",
        "          break\n",
        "      #Append it to the key. Remove the first element from the key\n",
        "      text.append(word)\n",
        "      decodeKey.append(word)\n",
        "      decodeKey.pop(0)\n",
        "      decodeIndex = decodeIndex + 1"
      ],
      "metadata": {
        "id": "bw-n5zHtIVLC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decode(decodeIndexG35, codeG35, decodekeyG35, textG35, inputG35)\n",
        "print(textG35)\n",
        "Decode(decodeIndexG2, codeG2, decodekeyG2, textG2, inputG2)\n",
        "print(textG2)\n",
        "Decode(decodeIndexH, codeH, decodekeyH, textH, inputH)\n",
        "print(textH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x0AGtQ7duQm",
        "outputId": "68bf32a3-00d8-4154-adba-eb413dbbcc57"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Сонячний', 'ліс', 'влітку', 'був', 'найкращим', 'місцем', 'для', 'відпочинку', 'Під', 'високими', 'кронами', 'дерев', 'розквітали', 'барвисті', 'квіти', 'гілки', \"м'яко\", 'похитувалися', 'на', 'вітрі', 'Коли', 'я', 'вирушив', 'на', 'свою', 'прогулянку', 'сонце', 'щедро', 'розпростерло', 'свої', 'промені', 'створюючи', 'мерехтливі', 'плями', 'на', 'стежці', 'Початкові', 'кроки', 'привели', 'мене', 'до', 'гаю', 'з', 'величезними', 'дубами', 'і', 'кленами', 'Легкий', 'шум', 'листя', 'надавав', 'лісу', 'власний', 'ритм', 'а', 'спів', 'пташок', 'створював', 'мелодійну', 'симфонію', 'Змінюючи', 'напрямок', 'руху', 'я', 'опинився', 'біля', 'маленького', 'струмка', 'Вода', 'пливла', 'спокійно', 'і', 'чисто', 'а', 'на', 'його', 'березі', 'росли', 'жовті', 'лілії', 'що', 'розцвітали', 'в', 'воді', 'Пройшовши', 'глибше', 'в', 'ліс', 'я', 'натрапив', 'на', 'лісовий', 'майданчик', 'де', 'сонячні', 'промені', 'змішувалися', 'з', 'тінями', 'дерев', 'Тут', 'я', 'вирішив', 'влаштувати', 'пікнік', 'розклавши', 'ковдру', 'і', 'розмістивши', 'корзину', 'з', 'їжею', 'Затишна', 'атмосфера', 'і', 'шум', 'природи', 'зробили', 'мій', 'обід', 'надзвичайно', 'приємним', 'Найбільше', 'мене', 'вразила', 'зустріч', 'з', 'дикою', 'природою', 'Барвисті', 'метелики', 'летіли', 'навколо', 'мене', 'а', 'білки', 'стежили', 'за', 'моїми', 'рухами', 'Поруч', 'із', 'струмком', 'я', 'випив', 'прохолодної', 'води', 'і', 'відчував', 'себе', 'щасливим', 'Сонце', 'спускалося', 'за', 'горизонт', 'і', 'коли', 'я', 'повертався', 'додому', 'усе', 'ще', 'чув', 'спів', 'пташок', 'у', 'лісі', 'Та', 'прогулянка', 'лісом', 'влітку', 'залишила', 'в', 'мене', 'незабутні', 'враження', 'і', 'нагадала', 'наскільки', 'важливо', 'дбати', 'про', 'нашу', 'природу', 'і', 'насолоджуватися', 'її', 'красою']\n",
            "['Сонячний', 'літній', 'ліс', 'в', 'якому', 'я', 'люблю', 'гуляти', '–', 'це', 'місце', 'куди', 'хочеться', 'повертатися', 'знову', 'знову', 'Я', 'ніколи', 'не', 'забуду', 'цього', 'чарівного', 'місця', 'де', 'я', 'народилася', 'і', 'виросла', 'Тут', 'я', 'знайшла', 'себе', 'тут', 'я', 'зустріла', 'свою', 'другу', 'половинку', 'Я', 'завжди', 'мріяла', 'про', 'те', 'щоб', 'у', 'мене', 'була', 'своя', 'сім’я', 'Але', 'коли', 'я', 'стала', 'дорослою', 'я', 'зрозуміла', 'що', 'це', 'не', 'так', 'просто', 'І', 'мені', 'захотілося', 'зробити', 'щось', 'більше', 'ніж', 'просто', 'бути', 'мамою', 'У', 'мене', 'з’явилося', 'багато', 'друзів', 'з', 'якими', 'я', 'можу', 'поділитися', 'своїми', 'захопленнями', 'своїми', 'думками', 'і', 'переживаннями', 'Мені', 'дуже', 'подобається', 'подорожувати', 'і', 'я', 'дуже', 'рада', 'коли', 'мої', 'друзі', 'приїжджають', 'до', 'мене', 'в', 'гості', 'Коли', 'я', 'приїжджаю', 'до', 'них', 'мені', 'хочеться', 'поділитися', 'з', 'ними', 'своєю', 'любов’ю', 'і', 'турботою', 'про', 'мене', 'Тому', 'я', 'завжди', 'буду', 'рада', 'знайомству', 'з', 'новими', 'людьми', 'які', 'захочуть', 'поділитися', 'своїм', 'теплом', 'і', 'ніжністю', 'з', 'моїми', 'близькими', 'людьми', 'Це', 'дуже', 'важливо', 'для', 'мене', 'тому', 'що', 'я', 'відчуваю', 'як', 'мої', 'близькі', 'люди', 'відчувають', 'мою', 'любов', 'і', 'турботу', 'А', 'це', 'дуже', 'приємно', 'для', 'нас', 'обох', 'Адже', 'ми', 'любимо', 'один', 'одного', 'і', 'дуже', 'хочемо', 'бути', 'разом', 'Якщо', 'у', 'вас', 'є', 'діти', 'ви', 'завжди', 'можете', 'їм', 'допомогти', 'якщо', 'у', 'них', 'є', 'проблеми', 'зі', 'здоров’ям', 'вони', 'завжди', 'можуть', 'прийти', 'до', 'вас', 'на', 'прийом', 'і', 'поговорити', 'з', 'вами', 'Ви', 'завжди', 'зможете', 'знайти', 'спільну', 'мову', 'з', 'дітьми', 'і', 'розповісти', 'про', 'свої', 'проблеми', 'і', 'проблеми', 'своїх', 'близьких', 'людей', 'Так', 'само', 'ви', 'можете', 'запросити', 'їх', 'до', 'себе', 'на', 'відпочинок', 'або', 'в', 'школу']\n",
            "['Ліс', 'прекрасний', 'у', 'будь-який', 'час', 'року', 'але', 'особливо', 'влітку', 'Коли', 'заходиш', 'у', 'нього', 'то', 'відразу', 'змінюється', 'настрій', 'стає', 'легше', 'дихати', 'У', 'спекотну', 'погоду', 'відчувається', 'приємна', 'прохолода', 'В', 'холодний', 'день', 'навпаки', 'нагріті', 'стовбури', 'дерев', 'віддають', 'своє', 'тепло', 'При', 'вході', 'відразу', 'ж', 'уся', 'увага', 'звертається', 'на', 'дерева', 'У', 'змішаному', 'лісі', 'можна', 'побачити', 'величні', 'дуби', 'стрункі', 'сосни', 'та', 'пірамідальні', 'ялини', 'Дуже', 'гарно', 'виглядають', 'білокорі', 'берези', 'вони', 'завжди', 'радісно', 'шелестять', 'листячком', 'на', 'вітрі', 'Під', 'деревами', 'навкруги', 'ростуть', 'чагарники', 'Всі', 'рослини', 'тягнуться', 'вгору', 'ближче', 'до', 'світла', 'й', 'сонця', 'Ліс', 'влітку', 'дуже', 'багатий', 'різноманітною', 'рослинністю', 'Всюди', 'висока', 'трава', 'Особливо', 'приємно', 'знайти', 'сонячну', 'галявину', 'яка', 'вся', 'в', 'квітах', 'конюшини', 'ромашки', 'гвоздики', 'кашок', 'Багато', 'квітучих', 'лікарських', 'рослин', 'звіробій', 'материнка', 'деревій', 'А', 'можна', 'й', 'наштовхнутися', 'на', 'таке', 'місце', 'де', 'навколо', 'червоніють', 'ягоди', 'суниці', 'Потрібно', 'тільки', 'не', 'лінуватися', 'щоб', 'її', 'збирати', 'У', 'густих', 'заростях', 'зустрічається', 'чорниця', 'Опівдні', 'температура', 'повітря', 'стає', 'високою', 'квітучі', 'трави', 'починають', 'пахнути', 'Дуже', 'приємно', 'лягти', 'серед', 'цих', 'квітів', 'та', 'вдихати', 'аромати', 'квітів', 'А', 'ще', '—', 'дивитися', 'як', 'по', 'блакитному', 'небу', 'пропливають', 'хмари', 'та', 'вгадувати', 'на', 'що', 'вони', 'схожі', 'Отже', 'ліс', 'влітку', '—', 'осередок', 'краси', 'та', 'єднання', 'з', 'природою', 'Обов’язково', 'відвідайте', 'це', 'зелене', 'царство!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, dataframe.shape[0] - 1):\n",
        "  #Get the text and init all the needed arrays\n",
        "  print(\"Text \" + str(i))\n",
        "  text = dataframe[1].iloc[i]\n",
        "  words = ClearElements(text.split())\n",
        "  prompt = words[:promptSizeUkr]\n",
        "  key = words[:promptSizeUkr]\n",
        "  code = words[:promptSizeUkr]\n",
        "  code.append(endOfPromptKey)\n",
        "  currentIndex = promptSizeUkr + 1\n",
        "  print(\"Number of words: \" + str(len(words)))\n",
        "  print(\"Number of characters: \" + str(len(text)))\n",
        "  #Analyze the text\n",
        "  print(\"Entropy: \" + str(ShannonEntropy(text)))\n",
        "  procText = engnlp(text)\n",
        "  sum = 0\n",
        "  for phrase in procText._.phrases:\n",
        "    sum = sum + phrase.rank\n",
        "  average = sum / len(procText._.phrases)\n",
        "  print(\"Average: \" + str(average))\n",
        "  listT = []\n",
        "  for phrase in procText._.phrases:\n",
        "    listT.append(phrase.rank)\n",
        "  print(\"Variance: \" + str(statistics.variance(listT)))\n",
        "  compressed = bz2.compress(text.encode())\n",
        "  print(\"Compression Rate (Original): \" + str(len(text)/len(compressed)))\n",
        "  #Encode the text\n",
        "  Encode(currentIndex, words, key, tUkr, mUkr, maxIndexUkr, code)\n",
        "  #Compress the text\n",
        "  stringToCompress = CombineWordsInString(code)\n",
        "  compressed = bz2.compress(stringToCompress.encode())\n",
        "  print(\"Number of characters (Encoded): \" + str(len(stringToCompress)))\n",
        "  print(\"Number of characters (Encoded, Compressed): \" + str(len(compressed)))\n",
        "  print(\"Compression Rate (Encoded): \" + str(len(stringToCompress)/len(compressed)))\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyC74Dafpvb4",
        "outputId": "3c40dfef-8787-4e7a-f506-45816da3e1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 0\n",
            "Number of words: 143\n",
            "Number of characters: 834\n",
            "Entropy: 4.642262413593914\n",
            "Average: 0.06948368037759553\n",
            "Variance: 0.0005617057589052754\n",
            "Compression Rate (Original): 1.5676691729323309\n",
            "Number of characters (Encoded): 699\n",
            "Number of characters (Encoded, Compressed): 359\n",
            "Compression Rate (Encoded): 1.947075208913649\n",
            "\n",
            "Text 1\n",
            "Number of words: 226\n",
            "Number of characters: 1272\n",
            "Entropy: 4.650908711830497\n",
            "Average: 0.060753819940388046\n",
            "Variance: 0.0008402505063645537\n",
            "Compression Rate (Original): 1.6649214659685865\n",
            "Number of characters (Encoded): 1003\n",
            "Number of characters (Encoded, Compressed): 485\n",
            "Compression Rate (Encoded): 2.0680412371134023\n",
            "\n",
            "Text 2\n",
            "Number of words: 182\n",
            "Number of characters: 962\n",
            "Entropy: 4.656544258581663\n",
            "Average: 0.06378857234543575\n",
            "Variance: 0.0007633565069896946\n",
            "Compression Rate (Original): 1.5441412520064206\n",
            "Number of characters (Encoded): 756\n",
            "Number of characters (Encoded, Compressed): 405\n",
            "Compression Rate (Encoded): 1.8666666666666667\n",
            "\n",
            "Text 3\n",
            "Number of words: 163\n",
            "Number of characters: 974\n",
            "Entropy: 4.780423424494503\n",
            "Average: 0.05594796515033597\n",
            "Variance: 0.0004622782873251644\n",
            "Compression Rate (Original): 1.5387045813586098\n",
            "Number of characters (Encoded): 790\n",
            "Number of characters (Encoded, Compressed): 410\n",
            "Compression Rate (Encoded): 1.9268292682926829\n",
            "\n",
            "Text 4\n",
            "Number of words: 206\n",
            "Number of characters: 1374\n",
            "Entropy: 4.792586628239196\n",
            "Average: 0.04890197410996272\n",
            "Variance: 0.0006982490155878899\n",
            "Compression Rate (Original): 1.5098901098901099\n",
            "Number of characters (Encoded): 1175\n",
            "Number of characters (Encoded, Compressed): 556\n",
            "Compression Rate (Encoded): 2.1133093525179856\n",
            "\n",
            "Text 5\n",
            "Number of words: 219\n",
            "Number of characters: 1221\n",
            "Entropy: 4.6732317569590895\n",
            "Average: 0.054434963057801315\n",
            "Variance: 0.0006702974507637611\n",
            "Compression Rate (Original): 1.6726027397260275\n",
            "Number of characters (Encoded): 778\n",
            "Number of characters (Encoded, Compressed): 395\n",
            "Compression Rate (Encoded): 1.969620253164557\n",
            "\n",
            "Text 6\n",
            "Number of words: 214\n",
            "Number of characters: 1204\n",
            "Entropy: 4.6495275499233895\n",
            "Average: 0.06258993635453186\n",
            "Variance: 0.0012020024356333813\n",
            "Compression Rate (Original): 1.7449275362318841\n",
            "Number of characters (Encoded): 999\n",
            "Number of characters (Encoded, Compressed): 474\n",
            "Compression Rate (Encoded): 2.107594936708861\n",
            "\n",
            "Text 7\n",
            "Number of words: 168\n",
            "Number of characters: 1064\n",
            "Entropy: 4.7989277281864196\n",
            "Average: 0.060215901877015206\n",
            "Variance: 0.0006104772952423345\n",
            "Compression Rate (Original): 1.5975975975975976\n",
            "Number of characters (Encoded): 635\n",
            "Number of characters (Encoded, Compressed): 362\n",
            "Compression Rate (Encoded): 1.7541436464088398\n",
            "\n",
            "Text 8\n",
            "Number of words: 207\n",
            "Number of characters: 1432\n",
            "Entropy: 4.707271919454856\n",
            "Average: 0.05392395120759155\n",
            "Variance: 0.0005371576272959983\n",
            "Compression Rate (Original): 1.6\n",
            "Number of characters (Encoded): 1102\n",
            "Number of characters (Encoded, Compressed): 517\n",
            "Compression Rate (Encoded): 2.1315280464216633\n",
            "\n",
            "Text 9\n",
            "Number of words: 198\n",
            "Number of characters: 1400\n",
            "Entropy: 4.745113417333031\n",
            "Average: 0.047544760536426166\n",
            "Variance: 0.0003801973970290054\n",
            "Compression Rate (Original): 1.6\n",
            "Number of characters (Encoded): 1055\n",
            "Number of characters (Encoded, Compressed): 527\n",
            "Compression Rate (Encoded): 2.001897533206831\n",
            "\n",
            "Text 10\n",
            "Number of words: 173\n",
            "Number of characters: 1241\n",
            "Entropy: 4.7151364988585405\n",
            "Average: 0.05085369153792432\n",
            "Variance: 0.0003539004940853035\n",
            "Compression Rate (Original): 1.5454545454545454\n",
            "Number of characters (Encoded): 1217\n",
            "Number of characters (Encoded, Compressed): 578\n",
            "Compression Rate (Encoded): 2.105536332179931\n",
            "\n",
            "Text 11\n",
            "Number of words: 192\n",
            "Number of characters: 1138\n",
            "Entropy: 4.65668068840868\n",
            "Average: 0.05670838370986536\n",
            "Variance: 0.0005292183350253273\n",
            "Compression Rate (Original): 1.598314606741573\n",
            "Number of characters (Encoded): 1112\n",
            "Number of characters (Encoded, Compressed): 534\n",
            "Compression Rate (Encoded): 2.0823970037453186\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I need to split the text into sentences AND IN UKRAINIAN\n",
        "#Library link: https://github.com/lang-uk/tokenize-uk/tree/master\n",
        "\n",
        "#!pip install tokenize_uk\n",
        "\n",
        "import tokenize_uk\n",
        "from tokenize_uk import tokenize_sents\n",
        "\n",
        "text = \"Перше речення. Друге речення. Речення №1.5. Речення ім. мене.\"\n",
        "sentences = tokenize_sents(text)\n",
        "for sent in sentences:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWwa1xLN7RQK",
        "outputId": "2af12610-17b2-41c1-998b-b039c108857f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Перше речення.\n",
            "Друге речення.\n",
            "Речення №1.5.\n",
            "Речення ім. мене.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What I also want to try is the Dynamic Key - encoding using SENTENCES and not WORDS\n",
        "\n",
        "#The text we're to encode\n",
        "text = \"І ось ми тут, робимо лабораторну роботу 1 українською мовою. На нас чекають ще інші справи, але ми тут і робимо це. А на додачу ще й ключі робимо з речень. Щоб життя зовсім легким не здавалося.\"\n",
        "sentenceArray = tokenize_sents(text)  #All sentences in the text\n",
        "keyNumber = 2  #How many sentences are to be used as a key\n",
        "#WE ASSUME the sentence array is LARGER than the prompt we need\n",
        "promptSentenceArray = sentenceArray[:keyNumber]  #The beginning of the text we're to keep safe\n",
        "sentencesKey = sentenceArray[:keyNumber]  #The sentences to act as the key - starts as a prompt, but transforms after each sentence encoded\n",
        "\n",
        "maxIndex = 200  #Max code we can insert in the code list\n",
        "\n",
        "#NEEDED encode parts of the prompt as well! Or is this a MAYBE?\n",
        "code = promptSentenceArray[:]  #Well, code itself - prompt is put here, and all other codes are too\n",
        "unknownKeyS = 253\n",
        "unknownKeyE = 254  #If the word doesn't appear in the tensor - we put it in the code, bracketing it with these two keys\n",
        "notFitKeyS = 251\n",
        "notFitKeyE = 252  #If the word appeared in tensor, but didn't fit within the code limit - we put it in code, bracketing with these two keys\n",
        "numberWordKeyS = 249\n",
        "numberWordKeyE = 250  #If the word is a number and doesn't have to be decrypted while decoding - we bracket it with these two keys\n",
        "endOfSentenceKey = 255  #When we work with sentences, we need to keep the delimiter\n",
        "\n",
        "print(sentenceArray)\n",
        "print(promptSentenceArray)\n",
        "print(sentencesKey)\n",
        "print(code)"
      ],
      "metadata": {
        "id": "l4vhwL_fJpDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5541ad87-c5d0-4ca8-dc7d-a8c32c3a0065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['І ось ми тут, робимо лабораторну роботу 1 українською мовою.', 'На нас чекають ще інші справи, але ми тут і робимо це.', 'А на додачу ще й ключі робимо з речень.', 'Щоб життя зовсім легким не здавалося.']\n",
            "['І ось ми тут, робимо лабораторну роботу 1 українською мовою.', 'На нас чекають ще інші справи, але ми тут і робимо це.']\n",
            "['І ось ми тут, робимо лабораторну роботу 1 українською мовою.', 'На нас чекають ще інші справи, але ми тут і робимо це.']\n",
            "['І ось ми тут, робимо лабораторну роботу 1 українською мовою.', 'На нас чекають ще інші справи, але ми тут і робимо це.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This one works WAAAAY too long. Guess I'll drop it for now\n",
        "currentIndex = keyNumber\n",
        "while currentIndex < len(sentenceArray):\n",
        "  stringKey = CombineSentencesToString(sentencesKey)\n",
        "  inSentenceIndex = 0\n",
        "  stringToEncode = stringKey\n",
        "  while inSentenceIndex < len(sentenceArray[currentIndex]):\n",
        "    words = ClearElements(sentenceArray[currentIndex].split())\n",
        "    stringToEncode = stringToEncode + \" \" + words[inSentenceIndex]\n",
        "    encoded_string = tUkr(stringToEncode, return_tensors=\"pt\")\n",
        "    wordsWithProbs = FindWordsDictionary(encoded_string, maxIndex * 2, mUkr, tUkr)\n",
        "\n",
        "    CodeAppending(words, wordsWithProbs, inSentenceIndex, maxIndex, code)\n",
        "  print(sentencesKey)\n",
        "  sentencesKey.pop(0)\n",
        "  sentencesKey.append(sentenceArray[currentIndex])\n",
        "  #Increase the currentIndex\n",
        "  currentIndex = currentIndex + 1\n",
        "  print(sentencesKey)\n",
        "  print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "cOm5taQm0p8t",
        "outputId": "c8ad88af-ea01-4443-cced-22eda995f93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-efaa977d03e4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstringToEncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringToEncode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minSentenceIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mencoded_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtUkr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringToEncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mwordsWithProbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFindWordsDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIndexUkr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmUkr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtUkr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mCodeAppending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordsWithProbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minSentenceIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0ac058629708>\u001b[0m in \u001b[0;36mFindWordsDictionary\u001b[0;34m(encoded_text, numOfWords, model, tokenizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#1. step to get the logits of the next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#print(next_token_logits.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#And now we need to compress the encoded thing\n",
        "import zlib\n",
        "\n",
        "def StringsToIntsInACode(codeParts):\n",
        "  counter = 0\n",
        "  for codePart in codeParts:\n",
        "    if codePart.isdigit():\n",
        "      codeParts[counter] = int(codePart)\n",
        "    counter = counter + 1\n",
        "  return codeParts\n",
        "\n",
        "#Darn, to compress, I need to put the code in a string\n",
        "stringToCompressUkr = CombineWordsInString(codeG35)\n",
        "print(stringToCompressUkr)\n",
        "compressed = zlib.compress(stringToCompressUkr.encode())\n",
        "print(compressed)\n",
        "print(len(compressed))\n",
        "decompressed = zlib.decompress(compressed).decode()\n",
        "print(decompressed)\n",
        "print(len(decompressed))\n",
        "codeMade = decompressed.split()\n",
        "print(codeMade)\n",
        "#Not much of a compression...\n",
        "#Maybe I need a more powerful library?\n",
        "\n",
        "print(StringsToIntsInACode(codeMade))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU8_CaSIJzok",
        "outputId": "00c22c69-77b3-4c72-8987-6dd6ed8ab5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "І ось ми тут робимо лабораторну роботу 250 n1 251 254 українською 255 0 208 254 нас 255 1 50 57 46 123 1 68 1 58 4\n",
            "b\"x\\x9c5\\x8c\\xc1\\r\\xc3 \\x10\\x04[\\xd9\\x12\\x80\\x80MU\\xa9!\\x06\\xc9\\xaf(\\xdf\\xb4a\\xff,'\\xa6\\x86\\xbd\\x8e\\xb2 \\xe5qZ\\xed\\xdc\\xdcq\\x05\\x9b-\\xf6\\x04\\xbf<`\\xc5\\xaa\\x15\\xd8\\x83\\x8d;\\x0f\\xb1\\x06~\\xb8\\xa94\\xc1\\xcd\\xca\\xc8\\xcb\\xea\\xdfi\\xfd\\x04!9\\xdc\\xbd\\xa2O\\x84U\\x9eC\\x7fK\\xd5s\\x9e\\xf2^Z%8\\x04\\x97\\x87\\xc4K\\xc22\\xa0\\x87\\xee\\xd3\\x8c8\\xc1\\x87\\x9b\\xea\\x94;\\xcb\\x88?(\\xe2N9\"\n",
            "120\n",
            "І ось ми тут робимо лабораторну роботу 250 n1 251 254 українською 255 0 208 254 нас 255 1 50 57 46 123 1 68 1 58 4\n",
            "114\n",
            "['І', 'ось', 'ми', 'тут', 'робимо', 'лабораторну', 'роботу', '250', 'n1', '251', '254', 'українською', '255', '0', '208', '254', 'нас', '255', '1', '50', '57', '46', '123', '1', '68', '1', '58', '4']\n",
            "['І', 'ось', 'ми', 'тут', 'робимо', 'лабораторну', 'роботу', 250, 'n1', 251, 254, 'українською', 255, 0, 208, 254, 'нас', 255, 1, 50, 57, 46, 123, 1, 68, 1, 58, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What I want to try is detect not only Integers, but also Floats and keep them unchanged\n",
        "import re\n",
        "\n",
        "def KeepFloats(inputParts):\n",
        "  outputParts = []\n",
        "  for part in inputParts:\n",
        "    if re.match(r\"[-+]?(?:\\d*\\.*\\d+)\", part):\n",
        "      outputParts.append(\"n\" + part)\n",
        "    else:\n",
        "      outputParts.append(part)\n",
        "  return outputParts\n",
        "\n",
        "floatString = \"One of my friends is 24.5 years old and another one is 20 years old\"\n",
        "floatStringParts = floatString.split()\n",
        "returnedParts = KeepFloats(floatStringParts)\n",
        "print(returnedParts)\n",
        "\n",
        "clearedParts = ClearElements(floatStringParts)\n",
        "print(clearedParts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQccnzJ4wqeg",
        "outputId": "8e4bb8e6-b64a-4410-8b3b-b6e418ced2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'of', 'my', 'friends', 'is', 'n24.5', 'years', 'old', 'and', 'another', 'one', 'is', 'n20', 'years', 'old']\n",
            "['One', 'of', 'my', 'friends', 'is', '24.5', 'years', 'old', 'and', 'another', 'one', 'is', '20', 'years', 'old']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What I need is getting the probabilities of words occuring in the text\n",
        "\n",
        "def LetterProbabilities(inputString):\n",
        "  words = collections.Counter(inputString)\n",
        "  amount = words.total()\n",
        "  probs = {}\n",
        "  counter = 0\n",
        "  for word in words:\n",
        "    probs.update({word : words[word] / amount* 100})\n",
        "  return probs\n",
        "\n",
        "wordProbs = LetterProbabilities(text)\n",
        "for word in wordProbs:\n",
        "  print(word + ' | ' + str(wordProbs[word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFzh80H64PQK",
        "outputId": "3c6a88d8-2f82-498c-a0e9-63bba272cc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L | 0.4405286343612335\n",
            "e | 11.45374449339207\n",
            "t | 6.607929515418502\n",
            "’ | 0.4405286343612335\n",
            "s | 4.845814977973569\n",
            "  | 17.180616740088105\n",
            "a | 7.929515418502203\n",
            "y | 3.524229074889868\n",
            "w | 3.0837004405286343\n",
            "h | 3.9647577092511015\n",
            "v | 0.881057268722467\n",
            "l | 4.845814977973569\n",
            "n | 4.405286343612335\n",
            "g | 0.881057268722467\n",
            "u | 3.9647577092511015\n",
            "m | 0.881057268722467\n",
            "o | 4.405286343612335\n",
            "d | 2.643171806167401\n",
            "b | 1.762114537444934\n",
            "r | 3.0837004405286343\n",
            "i | 2.643171806167401\n",
            "c | 2.643171806167401\n",
            "f | 1.762114537444934\n",
            "5 | 0.4405286343612335\n",
            ", | 2.2026431718061676\n",
            ". | 0.881057268722467\n",
            "N | 0.4405286343612335\n",
            "p | 1.3215859030837005\n",
            "x | 0.4405286343612335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Okay, I get a list of letters. Now I'll try to get the WORDS\n",
        "#Need to remove the spaces as well\n",
        "import re\n",
        "\n",
        "delimiters = [',', '|', ';', '.', ':', ' ']\n",
        "\n",
        "def WordProbabilities(inputString):\n",
        "  pattern = r'[,|;.: ]'\n",
        "  allWords = re.split(pattern, inputString)\n",
        "  words = collections.Counter(allWords)\n",
        "  for word in words:\n",
        "    if word in delimiters:\n",
        "      del words[word]\n",
        "  amount = words.total()\n",
        "  probs = {}\n",
        "  for word in words:\n",
        "      probs.update({word : words[word] / amount * 100})\n",
        "  return probs\n",
        "\n",
        "wordProbs = WordProbabilities(text)\n",
        "for word in wordProbs:\n",
        "  print(word + ' | ' + str(wordProbs[word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMTOks30_cxr",
        "outputId": "5f843e63-8b37-4d24-ef35-bce11f62f3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let’s | 2.127659574468085\n",
            "say | 2.127659574468085\n",
            "we | 4.25531914893617\n",
            "have | 2.127659574468085\n",
            "a | 4.25531914893617\n",
            "language | 2.127659574468085\n",
            "model | 4.25531914893617\n",
            "that | 2.127659574468085\n",
            "has | 2.127659574468085\n",
            "been | 2.127659574468085\n",
            "trained | 2.127659574468085\n",
            "with | 2.127659574468085\n",
            "vocabulary | 2.127659574468085\n",
            "of | 4.25531914893617\n",
            "only | 2.127659574468085\n",
            "5 | 2.127659574468085\n",
            "words | 2.127659574468085\n",
            "sunny | 2.127659574468085\n",
            " | 14.893617021276595\n",
            "day | 2.127659574468085\n",
            "beautiful | 4.25531914893617\n",
            "scenery | 4.25531914893617\n",
            "clouds | 2.127659574468085\n",
            "Now | 2.127659574468085\n",
            "want | 2.127659574468085\n",
            "to | 2.127659574468085\n",
            "calculate | 2.127659574468085\n",
            "the | 6.382978723404255\n",
            "perplexity | 2.127659574468085\n",
            "when | 2.127659574468085\n",
            "it | 2.127659574468085\n",
            "sees | 2.127659574468085\n",
            "phrase | 2.127659574468085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I'll try a bit different approach\n",
        "#YAAAY ^-^\n",
        "#Maybe also make the IgnoreCase? DONE\n",
        "import math\n",
        "\n",
        "delimiters = [',', '|', ';', '.', ':', ' ']\n",
        "\n",
        "def CountElements(elements):\n",
        "  dictionary = {}\n",
        "  for element in elements:\n",
        "    element = element.lower()\n",
        "    if element[-1] in delimiters:\n",
        "        element = element[:-1]\n",
        "    if element in dictionary:\n",
        "        dictionary[element] += 1\n",
        "    else:\n",
        "        dictionary.update({element: 1})\n",
        "  return dictionary\n",
        "\n",
        "def WordProbabilities2(inputString):\n",
        "  words = CountElements(inputString.split())\n",
        "  counter = 0\n",
        "  amount = math.fsum(words.values())\n",
        "  probs = {}\n",
        "  for word in words:\n",
        "      probs.update({word : words[word] / amount * 100})\n",
        "  return probs\n",
        "\n",
        "wordProbs = WordProbabilities2(text)\n",
        "sortedWordProbs = dict(sorted(wordProbs.items(), key=lambda x:x[1]))\n",
        "for word in sortedWordProbs:\n",
        "  print(word + ' | ' + str(sortedWordProbs[word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGmlfqAzIKke",
        "outputId": "fc63c287-214e-4d20-af28-7cb4350c375a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let’s | 2.5\n",
            "say | 2.5\n",
            "have | 2.5\n",
            "language | 2.5\n",
            "that | 2.5\n",
            "has | 2.5\n",
            "been | 2.5\n",
            "trained | 2.5\n",
            "with | 2.5\n",
            "vocabulary | 2.5\n",
            "only | 2.5\n",
            "5 | 2.5\n",
            "words | 2.5\n",
            "sunny | 2.5\n",
            "day | 2.5\n",
            "clouds | 2.5\n",
            "now | 2.5\n",
            "want | 2.5\n",
            "to | 2.5\n",
            "calculate | 2.5\n",
            "perplexity | 2.5\n",
            "when | 2.5\n",
            "it | 2.5\n",
            "sees | 2.5\n",
            "phrase | 2.5\n",
            "we | 5.0\n",
            "a | 5.0\n",
            "model | 5.0\n",
            "of | 5.0\n",
            "beautiful | 5.0\n",
            "scenery | 5.0\n",
            "the | 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Maybe what I'll use is Prediction by Partial Matching\n",
        "#But I'll start with Huffman coding. I've used it once\n",
        "#Link where I've taken the code from: https://favtutor.com/blogs/huffman-coding\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class NodeTree(object):\n",
        "    def __init__(self, left=None, right=None):\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "    def children(self):\n",
        "        return self.left, self.right\n",
        "    def __str__(self):\n",
        "        return self.left, self.right\n",
        "\n",
        "\n",
        "def huffman_code_tree(node, binString=''):\n",
        "    if type(node) is str:\n",
        "        return {node: binString}\n",
        "    (l, r) = node.children()\n",
        "    d = dict()\n",
        "    d.update(huffman_code_tree(l, binString + '0'))\n",
        "    d.update(huffman_code_tree(r, binString + '1'))\n",
        "    return d\n",
        "\n",
        "\n",
        "def make_tree(nodes):\n",
        "    while len(nodes) > 1:\n",
        "        (key1, c1) = nodes[-1]\n",
        "        (key2, c2) = nodes[-2]\n",
        "        nodes = nodes[:-2]\n",
        "        node = NodeTree(key1, key2)\n",
        "        nodes.append((node, c1 + c2))\n",
        "        nodes = sorted(nodes, key=lambda x: x[1], reverse=True)\n",
        "    return nodes[0][0]\n",
        "\n",
        "text = 'BCAADDDCCACACAC'\n",
        "freq = dict(Counter(text))\n",
        "freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
        "node = make_tree(freq)\n",
        "encoding = huffman_code_tree(node)\n",
        "for i in encoding:\n",
        "    print(f'{i} : {encoding[i]}')"
      ],
      "metadata": {
        "id": "tUX9gtwgvm_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfe6e1f-13bb-4da7-de2d-9e03a3bf016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C : 0\n",
            "B : 100\n",
            "D : 101\n",
            "A : 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One more text to test out\n",
        "\n",
        "text = \"Let’s say we have a language model that has been trained with a vocabulary of only 5 words sunny, day, beautiful, scenery, clouds. Now, we want to calculate the perplexity of the model when it sees the phrase beautiful scenery.\"\n",
        "freq = dict(Counter(text))\n",
        "freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
        "node = make_tree(freq)\n",
        "encoding = huffman_code_tree(node)\n",
        "for i in encoding:\n",
        "    print(f'{i} : {encoding[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6NVSft0Fkjz",
        "outputId": "54ed8125-b5dd-4e35-90cf-74270890b531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o : 0000\n",
            "n : 0001\n",
            ", : 00100\n",
            "c : 00101\n",
            "l : 0011\n",
            "s : 0100\n",
            "i : 01010\n",
            "d : 01011\n",
            "e : 011\n",
            "p : 100000\n",
            "m : 1000010\n",
            "g : 1000011\n",
            "r : 10001\n",
            "w : 10010\n",
            "x : 10011000\n",
            "N : 10011001\n",
            ". : 1001101\n",
            "f : 100111\n",
            "t : 1010\n",
            "y : 10110\n",
            "b : 101110\n",
            "v : 1011110\n",
            "L : 10111110\n",
            "5 : 101111110\n",
            "’ : 101111111\n",
            "u : 11000\n",
            "h : 11001\n",
            "a : 1101\n",
            "  : 111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pretty nice\n",
        "#Guess I need to enter the deep water now. The PPM algorithm\n",
        "\n",
        "#I feel like there's a library I can use...\n",
        "#It's called arithmetic_compressor\n",
        "#Link: https://pypi.org/project/arithmetic-compressor/\n",
        "#Or there is another one, pyppmd, Link: https://pyppmd.readthedocs.io/en/latest/\n",
        "\n",
        "#Guess I need a sort-of different prediction model. I need to predict WORDS, not letters, as in the regular model\n",
        "\n",
        "!pip install arithmetic_compressor\n",
        "from arithmetic_compressor import AECompressor\n",
        "from arithmetic_compressor.models import StaticModel\n",
        "\n",
        "letterProbs = LetterProbabilities(text)\n",
        "model = StaticModel(letterProbs)\n",
        "\n",
        "# create an arithmetic coder\n",
        "coder = AECompressor(model)\n",
        "\n",
        "# encode some data\n",
        "N = len(text)\n",
        "compressed = coder.compress(text)\n",
        "\n",
        "# print the compressed data\n",
        "print(compressed)\n",
        "print(len(compressed))\n",
        "\n",
        "#now decode\n",
        "decoded = coder.decompress(compressed, N)\n",
        "print(decoded)\n",
        "print(len(decoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsz3tEkLF7ti",
        "outputId": "2e3a20e2-dd70-4c69-ba0f-10176b8011fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arithmetic_compressor\n",
            "  Downloading arithmetic_compressor-0.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: arithmetic_compressor\n",
            "Successfully installed arithmetic_compressor-0.2\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1]\n",
            "963\n",
            "['L', 'e', 't', '’', 's', ' ', 's', 'a', 'y', ' ', 'w', 'e', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 't', 'h', 'a', 't', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'v', 'o', 'c', 'a', 'b', 'u', 'l', 'a', 'r', 'y', ' ', 'o', 'f', ' ', 'o', 'n', 'l', 'y', ' ', '5', ' ', 'w', 'o', 'r', 'd', 's', ' ', 's', 'u', 'n', 'n', 'y', ',', ' ', 'd', 'a', 'y', ',', ' ', 'b', 'e', 'a', 'u', 't', 'i', 'f', 'u', 'l', ',', ' ', 's', 'c', 'e', 'n', 'e', 'r', 'y', ',', ' ', 'c', 'l', 'o', 'u', 'd', 's', '.', ' ', 'N', 'o', 'w', ',', ' ', 'w', 'e', ' ', 'w', 'a', 'n', 't', ' ', 't', 'o', ' ', 'c', 'a', 'l', 'c', 'u', 'l', 'a', 't', 'e', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'r', 'p', 'l', 'e', 'x', 'i', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 's', 'e', 'e', 's', ' ', 't', 'h', 'e', ' ', 'p', 'h', 'r', 'a', 's', 'e', ' ', 'b', 'e', 'a', 'u', 't', 'i', 'f', 'u', 'l', ' ', 's', 'c', 'e', 'n', 'e', 'r', 'y', '.']\n",
            "227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Allright, we have the encoded letters. I doubt I can encode words using my text, I need a larger one\n",
        "#BUT I can test the library on letters first\n",
        "from arithmetic_compressor.models import PPMModel, MultiPPM\n",
        "\n",
        "letters = Counter(text)\n",
        "model = PPMModel(letters, k = 5) # no need to pass in probabilities, only symbols\n",
        "\n",
        "# create an arithmetic coder\n",
        "coder = AECompressor(model)\n",
        "\n",
        "# encode some data\n",
        "data = text[0: len(text) // 2]\n",
        "compressed = coder.compress(data)\n",
        "\n",
        "# print the compressed data\n",
        "print(compressed)\n",
        "print(len(compressed))\n",
        "\n",
        "#now decode\n",
        "decoded = coder.decompress(compressed, len(text)//2)\n",
        "print(decoded)\n",
        "print(len(decoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtcHw60_VuQ-",
        "outputId": "396b4cc4-b8fd-4795-a3f6-90f9db456e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
            "546\n",
            "['L', 'e', 't', '’', 's', ' ', 's', 'a', 'y', ' ', 'w', 'e', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 't', 'h', 'a', 't', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'v', 'o', 'c', 'a', 'b', 'u', 'l', 'a', 'r', 'y', ' ', 'o', 'f', ' ', 'o', 'n', 'l', 'y', ' ', '5', ' ', 'w', 'o', 'r', 'd', 's', ' ', 's', 'u', 'n', 'n', 'y', ',', ' ', 'd', 'a', 'y', ',', ' ', 'b', 'e', 'a', 'u', 't', 'i', 'f', 'u', 'l', ',']\n",
            "113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The previous model gives an error if it doesn't know the symbol it gets. The context there is strict. I need a more flexible version\n",
        "#BUUUUT if we get a long text, there will be all letters\n",
        "#But not all words, though, this is troubling\n",
        "\n",
        "#I need to try to predict the next word. There are some methods online to try out\n",
        "#I'll use this link as a code source: https://www.ris-ai.com/predict-next-word-with-python\n",
        "#Link to the dataset: https://www.kaggle.com/datasets/rowemorehouse/googleplaystoreuserreviews\n",
        "\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "dataset = pd.read_csv('/content/gdrive/MyDrive/Kaggle/googleplaystore_user_reviews.csv')\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "mJuUiSCXY3tZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "57ff9e04-50fd-4eb9-b48f-4020fc0c0d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     App                                  Translated_Review  \\\n",
              "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
              "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
              "2  10 Best Foods for You                                                NaN   \n",
              "3  10 Best Foods for You         Works great especially going grocery store   \n",
              "4  10 Best Foods for You                                       Best idea us   \n",
              "\n",
              "  Sentiment  Sentiment_Polarity  Sentiment_Subjectivity  \n",
              "0  Positive                1.00                0.533333  \n",
              "1  Positive                0.25                0.288462  \n",
              "2       NaN                 NaN                     NaN  \n",
              "3  Positive                0.40                0.875000  \n",
              "4  Positive                1.00                0.300000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56f968a1-f508-491c-af17-1f84628bc54d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>App</th>\n",
              "      <th>Translated_Review</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_Polarity</th>\n",
              "      <th>Sentiment_Subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10 Best Foods for You</td>\n",
              "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10 Best Foods for You</td>\n",
              "      <td>This help eating healthy exercise regular basis</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.288462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10 Best Foods for You</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10 Best Foods for You</td>\n",
              "      <td>Works great especially going grocery store</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10 Best Foods for You</td>\n",
              "      <td>Best idea us</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56f968a1-f508-491c-af17-1f84628bc54d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56f968a1-f508-491c-af17-1f84628bc54d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56f968a1-f508-491c-af17-1f84628bc54d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9719d15-5ee3-4860-9515-7e0c61efe794\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9719d15-5ee3-4860-9515-7e0c61efe794')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9719d15-5ee3-4860-9515-7e0c61efe794 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPart = pd.Series(dataset['Translated_Review'], dtype=str)\n",
        "datasetPart.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_giPl6HGWnAR",
        "outputId": "737b6bb1-6a5a-4b2a-9a28-f5d767669729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I like eat delicious food. That's I'm cooking ...\n",
              "1      This help eating healthy exercise regular basis\n",
              "2                                                  nan\n",
              "3           Works great especially going grocery store\n",
              "4                                         Best idea us\n",
              "Name: Translated_Review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = []\n",
        "for string in datasetPart:\n",
        "  words.extend(tokenizer.tokenize(string))\n",
        "unique_words = np.unique(words)\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ],
      "metadata": {
        "id": "R1d84uJ8btqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "    prev_words.append(words[i:i + WORD_LENGTH])\n",
        "    next_words.append(words[i + WORD_LENGTH])\n",
        "print(prev_words[0])\n",
        "print(next_words[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqL8ZGb-dqtZ",
        "outputId": "fa3cfa6e-32bc-4afc-aa86-2142d544c329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'like', 'eat', 'delicious', 'food']\n",
            "That\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ERROR out of RAM\n",
        "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)\n",
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1\n",
        "    Y[i, unique_word_index[next_words[i]]] = 1\n",
        "print(X[0][0])"
      ],
      "metadata": {
        "id": "jq-fwrM-PCGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What's the problem with finding the word?\n",
        "wordUkr = 'мені'\n",
        "dictionary = {tUkr.decode(idx): prob for idx, prob in zip(topk_next_tokensUkr.indices, topk_next_tokensUkr.values)}\n",
        "clearedKeys = []\n",
        "for item in dictionary.keys():\n",
        "  clearedKeys.append(ClearString(item))\n",
        "dictionary = dict(zip(clearedKeys, dictionary.values()))\n",
        "if wordUkr in dictionary.keys():\n",
        "  print(\"Got the key\")\n",
        "else:\n",
        "  print(\"Not the key\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdiZCkHAHG_4",
        "outputId": "6c00d1b1-d8c8-454c-fb69-a31049ae0966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got the key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I'll try to make the code\n",
        "\n",
        "promptSize = 7  #How long do we want our prompt to be - how many words are to be inserted\n",
        "maxIndex = 250  #Max code we can insert in the code list\n",
        "\n",
        "#The text we're to encode\n",
        "text = \"This used to be ok. Now there are way too many characters but nothing to do.\"\n",
        "#Second part of the text: Too many attractions but nowhere to put them. Developers have fallen way behind opening an additional land.\n",
        "words = ClearElements(text.split())\n",
        "prompt = words[:promptSize]  #The beginning of the text we'll keep safe for decoding\n",
        "key = words[:promptSize]  #The key - starts as a prompt, but transforms after each word encoded\n",
        "#NEEDED encode parts of the prompt as well! Or is this a MAYBE?\n",
        "code = words[:promptSize]  #Well, code itself - prompt is put here, and all other codes are too\n",
        "unknownKeyS = 254\n",
        "unknownKeyE = 255  #If the word doesn't appear in the tensor - we put it in the code, bracketing it with these two keys\n",
        "notFitKeyS = 252\n",
        "notFitKeyE = 253  #If the word appeared in tensor, but didn't fit within the 250-code limit - we put it in code, bracketing with these two keys\n",
        "numberWordKeyS = 250\n",
        "numberWordKeyE = 251  #If the word is a number and doesn't have to be decrypted while decoding\n",
        "\n",
        "currentIndex = promptSize\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsfSh_XfCpwn",
        "outputId": "1ab642e6-ea03-44f7-c5a8-fcfb1752adb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'used', 'to', 'be', 'ok', 'Now', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "m = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "encoded_text = t(\"I enjoy walking in the\", return_tensors=\"pt\")\n",
        "\n",
        "#1. step to get the logits of the next token\n",
        "with torch.inference_mode():\n",
        "  outputs = m(**encoded_text)\n",
        "\n",
        "next_token_logits = outputs.logits[0, -1, :]\n",
        "print(next_token_logits.shape)\n",
        "\n",
        "# 2. step to convert the logits to probabilities\n",
        "next_token_probs = torch.softmax(next_token_logits, -1)\n",
        "\n",
        "# 3. step to get the top 10\n",
        "topk_next_tokens= torch.topk(next_token_probs, 20)\n",
        "\n",
        "#putting it together\n",
        "print(*[(t.decode(idx), prob) for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)], sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcKKas5UCdNj",
        "outputId": "dfe58a9e-97c4-474b-b73b-50f42466e779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257])\n",
            "(' park', tensor(0.1590))\n",
            "(' woods', tensor(0.1003))\n",
            "(' streets', tensor(0.0418))\n",
            "(' dark', tensor(0.0312))\n",
            "(' door', tensor(0.0296))\n",
            "(' street', tensor(0.0239))\n",
            "(' rain', tensor(0.0217))\n",
            "(' city', tensor(0.0189))\n",
            "(' same', tensor(0.0150))\n",
            "(' halls', tensor(0.0135))\n",
            "(' field', tensor(0.0128))\n",
            "(' middle', tensor(0.0124))\n",
            "(' garden', tensor(0.0106))\n",
            "(' neighborhood', tensor(0.0103))\n",
            "(' snow', tensor(0.0095))\n",
            "(' forest', tensor(0.0092))\n",
            "(' parks', tensor(0.0090))\n",
            "(' open', tensor(0.0085))\n",
            "(' world', tensor(0.0076))\n",
            "(' hallway', tensor(0.0069))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while currentIndex < len(words):\n",
        "  string = CombineWordsInString(key)\n",
        "  encoded_text = t(string, return_tensors=\"pt\")\n",
        "\n",
        "  wordsWithProbs = FindWordsDictionary(encoded_text, maxIndex + 1, m, t)\n",
        "  CodeAppending(words, wordsWithProbs, currentIndex, maxIndex, code)\n",
        "  print(key)\n",
        "  #Now we got the word and added it to the code. We need to...\n",
        "  #Remove the first element from the key\n",
        "  key.pop(0)\n",
        "  #Add the word to the key\n",
        "  key.append(words[currentIndex])\n",
        "  #Increase the currentIndex\n",
        "  currentIndex = currentIndex + 1\n",
        "  #And repeat the process again\n",
        "  print(key)\n",
        "  print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "YG6FFQvIEN1g",
        "outputId": "80a4c3e1-5680-4e05-b217-460a9d34d2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-a8c8833c660a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mwordsWithProbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFindWordsDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIndex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mCodeAppending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordsWithProbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#Now we got the word and added it to the code. We need to...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-5d522342bd2f>\u001b[0m in \u001b[0;36mCodeAppending\u001b[0;34m(words, wordsWithProbs, currentIndex, maxIndex, code)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0misWordPresentInVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentIndex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordsWithProbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misWordPresentInVector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwordCode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsWithProbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumberWordKeyS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This was the ENCODING part\n",
        "#Now I need the DECODING part as well. This is where the prompt will be used\n",
        "\n",
        "input = code[:]\n",
        "text = prompt[:]\n",
        "decodeKey = prompt[:]\n",
        "decodeIndex = promptSize\n",
        "print(input)\n",
        "print(text)\n",
        "\n",
        "while decodeIndex < len(code):\n",
        "  if code[decodeIndex] == unknownKeyS or code[decodeIndex] == notFitKeyS or code[decodeIndex] == numberWordKeyS:\n",
        "    text.append(code[decodeIndex + 1])\n",
        "    decodeKey.append(code[decodeIndex + 1])\n",
        "    decodeKey.pop(0)\n",
        "    decodeIndex = decodeIndex + 3\n",
        "  else:\n",
        "    stringKey = CombineWordsInString(decodeKey)\n",
        "    encoded_string = t(stringKey, return_tensors=\"pt\")\n",
        "    possibleWords = FindWordsDictionary(encoded_string, maxIndex + 1, m, t)\n",
        "    index = input[decodeIndex]\n",
        "    text.append(list(possibleWords)[index])\n",
        "    decodeKey.append(list(possibleWords)[index])\n",
        "    decodeKey.pop(0)\n",
        "    decodeIndex = decodeIndex + 1\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ-IgUVQJGGQ",
        "outputId": "6a19807d-c9c6-4353-ae48-32ae2ea7183d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'used', 'to', 'be', 'ok', 'Now', 'there', 1, 90, 0, 0, 42, 56, 47, 3, 2]\n",
            "['This', 'used', 'to', 'be', 'ok', 'Now', 'there']\n",
            "['This', 'used', 'to', 'be', 'ok', 'Now', 'there', 'are', 'way', 'too', 'many', 'characters', 'but', 'nothing', 'to', 'do']\n"
          ]
        }
      ]
    }
  ]
}